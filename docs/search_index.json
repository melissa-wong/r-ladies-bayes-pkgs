[["index.html", "Intro to R Bayes Packages Preface", " Intro to R Bayes Packages Melissa Wong 2020-12-15 Preface TBD… "],["intro.html", "Chapter 1 Introduction 1.1 Resources 1.2 A Motivating Example 1.3 Workflow 1.4 Data 1.5 Prior Information", " Chapter 1 Introduction 1.1 Resources Bayesian Data Analysis by Andrew Gelman, et. al. A First Course in Bayesian Statistical Methods by Peter Hoff Statistical Rethinking by Richard McElreath 1.2 A Motivating Example Let’s start with something familiar–the Monty Hall problem. There are three doors labeled A, B and C. A car is behind one of the doors and a goat is behind each of the other two doors. You choose a door (let’s say A). Monty Hall, who knows where the car actually is, opens one of the other doors (let’s say B) revealing a goat. Do you stay with door A or do you switch to door C? We can frame the problem as follows: Initially, you believe the car is equally likely to behind each door (i.e., \\(P[A=car]=P[B=car]=P[C=car]=\\frac{1}{3}\\)). Let’s call this the prior information. Next, you can calculate the conditional probabilities that Monty Hall opened door B. Let’s call this the likelihood. \\[\\begin{align*} P[B=open | A=car] &amp;= \\frac{1}{2}\\\\ P[B=open | B=car] &amp;= 0 \\\\ P[B=open | C=car] &amp;=1 \\end{align*}\\] Finally, you can update your beliefs with the new information. Let’s call your updated beliefs the posterior. \\[\\begin{align*} P[A=car|B=open] &amp;= \\frac{P[B=open|A=car]P[A=car]}{P[B=open]} = \\frac{1/2 * 1/3}{1/6 + 0 + 1/3} = \\frac{1}{3} \\\\ P[B=car|B=open] &amp;= \\frac{P[B=open|B=car]P[B=car]}{P[B=open]} = \\frac{0 * 1/3}{1/6 + 0 + 1/3} = 0 \\\\ P[C=car|B=open] &amp;= \\frac{P[B=open|C=car]P[C=car]}{P[B=open]} = \\frac{1 * 1/3}{1/6 + 0 + 1/3} = \\frac{2}{3} \\end{align*}\\] Clearly, you should switch to door C. This is a toy illustration of how to think about a model in a Bayesian framework: \\[posterior \\propto likelihood * prior\\] (See the resources for a proper mathematical derivation.) 1.3 Workflow The workflow I’ll follow in the subsequent chapters is as follows: Define the model. Examine the prior predictive distribution. Examine diagnostic plots. Examine posterior distribution. Examine the posterior predictive distribution. In general, this is an iterative process. At each step you may discover something that causes you to start over at step 1 with a new, refined model. 1.4 Data For all of the examples, I use the mtcars data set and a model with disp as the predictor and mpg as the response. I start with a simple linear regression. However, as you can see from the scatterplot below, the relationship between mpg and disp is not linear, so I also fit a slightly more complex semi-parametric model. library(tidyverse) library(datasets) data(mtcars) mtcars %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_point() 1.5 Prior Information The mtcars dataset was extracted for Motor Trend magazine data on 1973-1974 models. The 1974 EPA Car Mileage Guide lists the mpg and engine size (i.e displacement) for U.S. cars and light duty trucks. The data is plotted below. library(gridExtra) epa &lt;- read.csv(&quot;./data/EPA_1974.csv&quot;, header=TRUE) p1 &lt;- ggplot() + geom_histogram(data=epa, mapping=aes(x=mpg), binwidth = 1) p2 &lt;- ggplot(epa) + geom_histogram(mapping=aes(x=Displacement), binwidth=20) p3 &lt;- ggplot(epa) + geom_point(mapping=aes(x=Displacement, y=mpg)) grid.arrange(p1, p2, ncol=2) p3 "],["rstanarm.html", "Chapter 2 rstanarm 2.1 Resources 2.2 Description 2.3 Environment Setup 2.4 Linear Model (Default Priors) 2.5 Linear Model (User-Defined Priors) 2.6 Semi-parametric Model 2.7 Session Info", " Chapter 2 rstanarm 2.1 Resources Regression and Other Stories by Gelman, Hill and Vehtari rstanarm online documentation -User-friendly Bayesian regression modeling: A tutorial with rstanarm and shinystan by Muth, Oravecz and Gabry 2.2 Description The rstanarm package is one of the easiest ways to get started with Bayesian models. The functions parallel the frequentist functions you’re probably already familiar with, and the syntax will also be familiar. You aren’t required to explicitly choose priors because all of the functions have weakly informative priors by default (although some might argue not being required to specify priors is a drawback). The primary limitation I’ve found thus far is the supported types for user-defined priors is somewhat limited. 2.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(gridExtra) library(kableExtra) library(datasets) data(mtcars) library(rstanarm) library(bayesplot) # Set number of cores options(mc.cores = parallel::detectCores()) 2.4 Linear Model (Default Priors) 2.4.1 Define Model Let’s start with the following simple linear model: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ \\end{align*}\\] The stan_glm function from the rstanarm package fits a Bayesian linear model. The syntax is very similar to lm/glm. mdl1 &lt;- stan_glm(mpg ~ disp, data = mtcars) 2.4.2 Prior Predictive Distribution Next, I’ll examine the prior predictive distribution to see if the default priors seem reasonable. The prior_summary function shows the default priors for the model as well as the adjusted priors after automatic scaling. See http://mc-stan.org/rstanarm/articles/priors.html if you are interested in the details about how the default and adjusted priors are calculated. prior_summary(mdl1) ## Priors for model &#39;mdl1&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 20, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 20, scale = 15) ## ## Coefficients ## Specified prior: ## ~ normal(location = 0, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0, scale = 0.12) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.17) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details Overlaying the default prior for the intercept with the EPA data gives a sense of what a weakly informative prior for this data looks like. # Plot expected value of prior predictive distribution using adjusted priors N &lt;- 100 prior_samples &lt;- data.frame(a = rnorm(N, 20, 15), b = rnorm(N, 0, 0.12)) D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) res &lt;- as.data.frame(apply(prior_samples, 1, function(x) x[1] + x[2] * (D-mean(mtcars$disp)))) %&gt;% mutate(disp = D) %&gt;% pivot_longer(cols=c(-&quot;disp&quot;), names_to=&quot;iter&quot;) res %&gt;% ggplot() + geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) + labs(x=&quot;disp&quot;, y=&quot;prior predictive mpg&quot;) I notice two things in the prior predictive distribution which seem unrealistic given what I know about the real world: 1) negative mpg and 2) increasing mpg as displacement increases. Later on I’ll choose a more informative prior which incorporates this external knowledge. But let’s proceed with the analysis and see what happens. 2.4.3 Diagnostics I’ll walk through the steps for manually extracting the key diagnostic information from the mdl1 object since I think that can be helpful to understand exactly what’s going on. However, once you have a handle on these steps I highly recommend the shinystan package; it will automatically create all of these diagnostic plots (and more) with an nice interactive web interface. 2.4.3.1 Trace Plots The bayesplot package provides the function mcmc_trace which plots the Markov Chain Monte Carlo (MCMC) draws. mcmc_trace(mdl1) There are three things I am looking for in the trace plot of each chain: Good mixing - In other words, the chain is rapidly changing values across the full region versus getting “stuck” near a particular value and slowly changing. Stationarity - The mean of the chain is relatively stable. Convergence - All of the chains spend most of the time around the same high-probability value. The trace plots above look good. 2.4.3.2 Trace Rank Plots It can sometimes be hard to interpret the trace plots when there are many chains. An alternative is the mcmc_rank_overlay function. This function plots a trace rank plot which is the distribution of the ranked samples; if the four chains have a roughly uniform distribution that indicates good mixing. mcmc_rank_overlay(mdl1) 2.4.3.3 \\(\\widehat{R}\\) and Effective Sample Size In addition to visually examining the chains, we should also check \\(\\widehat{R}\\) which is a measure of convergence. \\(\\widehat{R} &gt; 1.0\\) indicates poor mixing, and the mc_stan documentation recommends only using samples if \\(\\widehat{R} &lt; 1.05\\). However, a recent paper by Vehtari et al. (2020) recommends \\(\\widehat{R} &lt; 1.01\\). Since MCMC samples are usually correlated, the effective sample size (n_eff) is often less than the number of samples. There is no hard and fast rule for what is an acceptable number for n_eff. McElreath’s guidance is it depends on what you are trying to estimate. If you are interested mostly in the posterior mean, then n_eff = 200 can be enough. But if you are interested in the tails of the distribution and it’s highly skewed then you’ll need n_eff to be much larger. There are two parameters, iter and warmup, which you can adjust in stan_glm if a larger n_eff is needed. The summary function displays n_eff and \\(\\widehat{R}\\) for the object returned by stan_glm. summary(mdl1) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: mpg ~ disp ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 32 ## predictors: 2 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 29.6 1.3 27.9 29.6 31.2 ## disp 0.0 0.0 0.0 0.0 0.0 ## sigma 3.4 0.5 2.8 3.3 4.0 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 20.1 0.9 19.0 20.1 21.2 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3501 ## disp 0.0 1.0 3328 ## sigma 0.0 1.0 3056 ## mean_PPD 0.0 1.0 3411 ## log-posterior 0.0 1.0 1529 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). 2.4.4 Posterior Distribution Since the chains, n_eff and \\(\\widehat{R}\\) look good, let’s examine the posterior distributions next. # Posterior point estimates (medians are used for point estimates) coef(mdl1) ## (Intercept) disp ## 29.57206 -0.04103 # 95% credible intervals knitr::kable(posterior_interval(mdl1, prob=0.95)) 2.5% 97.5% (Intercept) 27.0319 32.1255 disp -0.0509 -0.0313 sigma 2.6281 4.3876 2.4.5 Posterior Predictive Distribution The posterior_predict function draws samples from the posterior predictive distribution, and then the ppc_dens_overlay function plots the distribution of each draw overlaid with the observed distribution. ppc_dens_overlay(mtcars$mpg, posterior_predict(mdl1, draws=50)) Below I also plot the expected value of the posterior predictive distribution and overlay the observations as an alternative way to visualize the result. The posterior_linpred function returns the linear predictor, possibly transformed by the inverse-link function. The posterior_epred function returns the expectation over the posterior predictive distribution. In this example, the model is a Gaussian likelihood with an identity link function, so the two functions return identical results. newdata &lt;- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp))) y_rep &lt;- as.data.frame(t(posterior_epred(mdl1, newdata=newdata, draws=50))) %&gt;% cbind(newdata) %&gt;% pivot_longer(cols=starts_with(&quot;V&quot;), names_to=&quot;grp&quot;, values_to=&quot;mpg&quot;) y_rep %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_line(aes(group=grp), alpha=0.2) + geom_point(data = mtcars) As expected, the linear model is not a good fit to the data. 2.5 Linear Model (User-Defined Priors) I’ll specify priors which incorporate the prior knowledge from the EPA data as well as that mpg is non-negative and is non-increasing as disp increases. My new model is as follows: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim Normal(13.2,5.3^2) \\\\ b &amp;\\sim Normal(-0.1, 0.05^2) \\\\ \\sigma &amp;\\sim Exponential(1) \\end{align*}\\] The differences from the default priors are The intercept prior is now set to the mean and standard deviation from the EPA data . The slope prior is no longer symmetric about 0, but rather it is centered at -0.2 so that positive values are less likely. (A prior distribution such as exponential or log-normal might be preferred in this case; however this is a limitation of rstanarm as those options aren’t available.) 2.5.1 Define Model mdl2 &lt;- stan_glm(mpg ~ disp, data = mtcars, prior = normal(-0.1, 0.05), # prior for slope prior_intercept = normal(13.2,5.3), # prior for intercept prior_aux = exponential(1)) # prior for standard deviation 2.5.2 Prior Predictive Distribution Below is an alternative to manually constructing the prior predictive distribution like I did in the previously. mdl2_prior &lt;- update(mdl2, prior_PD=TRUE, chains=1) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 7.5e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.75 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.030597 seconds (Warm-up) ## Chain 1: 0.022865 seconds (Sampling) ## Chain 1: 0.053462 seconds (Total) ## Chain 1: D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) draws &lt;- posterior_epred(mdl2_prior, newdata=data.frame(disp=D), draws=50) %&gt;% t() %&gt;% as.tibble() %&gt;% mutate(disp=D) %&gt;% pivot_longer(-disp, names_to=&quot;draw&quot;, values_to=&quot;mpg&quot;) ## Warning: `as.tibble()` is deprecated as of tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. draws %&gt;% ggplot() + geom_line(mapping=aes(x=disp, y=mpg, group=draw), alpha=0.2) 2.5.3 Diagnostics mcmc_rank_overlay(mdl2) summary(mdl2) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: mpg ~ disp ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 32 ## predictors: 2 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 29.7 1.2 28.1 29.7 31.2 ## disp 0.0 0.0 0.0 0.0 0.0 ## sigma 3.2 0.4 2.7 3.2 3.8 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 20.0 0.8 19.0 20.0 21.1 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3848 ## disp 0.0 1.0 3908 ## sigma 0.0 1.0 3673 ## mean_PPD 0.0 1.0 4013 ## log-posterior 0.0 1.0 1786 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). The trace plots, n_eff and \\(\\widehat{R}\\) all look good. 2.5.4 Posterior Distribution Now let’s compare the posterior with informative versus default priors: # Point estimates knitr::kable(cbind(coef(mdl1), coef(mdl2)), col.names = c(&quot;Default&quot;, &quot;Informative&quot;)) Default Informative (Intercept) 29.572 29.6711 disp -0.041 -0.0417 # 95% credible intervals knitr::kable(cbind(posterior_interval(mdl1, prob=0.95), posterior_interval(mdl2, prob=0.95))) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Default&quot; = 2, &quot;Informative&quot; = 2)) Default Informative 2.5% 97.5% 2.5% 97.5% (Intercept) 27.0319 32.1255 27.3210 32.0105 disp -0.0509 -0.0313 -0.0509 -0.0327 sigma 2.6281 4.3876 2.5380 4.1322 In this case, there is sufficient data that the choice of prior really didn’t make much of a difference. 2.5.5 Posterior Predictive Distribution ppc_dens_overlay(mtcars$mpg, posterior_predict(mdl2, draws=50)) # Expected value of posterior predictive newdata &lt;- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp))) y_rep &lt;- as.data.frame(t(posterior_epred(mdl2, newdata=newdata, draws=50))) %&gt;% cbind(newdata) %&gt;% pivot_longer(cols=starts_with(&quot;V&quot;), names_to=&quot;grp&quot;, values_to=&quot;mpg&quot;) y_rep %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_line(aes(group=grp), alpha=0.2) + geom_point(data = mtcars) The results are very similar to those with the default priors. 2.6 Semi-parametric Model 2.6.1 Define model The linear model is a poor choice for this data, so I’ll try a model with splines next. The stan_gamm4 function from the rstanarm package fits Bayesian nonlinear (and mixed) models. mdl3 &lt;- stan_gamm4(mpg ~ s(disp, bs=&quot;cr&quot;, k=7), data = mtcars, adapt_delta = 0.99) 2.6.2 Prior Predictive Distribution Unlike the linear model, it’s not as straightforward to manually construct the prior predictive distribution. Fortunately, rstanarm can automatically generate the prior predictive distribution too–we just refit the model without conditioning on the data by setting prior_PD = TRUE. mdl3_prior &lt;- update(mdl3, prior_PD = TRUE, chains=1) D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) draws &lt;- posterior_epred(mdl3_prior, newdata=data.frame(disp=D), draws=50) %&gt;% t() %&gt;% as.tibble() %&gt;% mutate(disp=D) %&gt;% pivot_longer(-disp, names_to=&quot;draw&quot;, values_to=&quot;mpg&quot;) draws %&gt;% ggplot() + geom_line(mapping=aes(x=disp, y=mpg, group=draw), alpha=0.2) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg), color=&quot;blue&quot;) This prior predictive distribution gives us some crazy possibilities. However we saw earlier that there is enough data that the model isn’t very sensitive to the choice of prior, so let’s continue and see what happens. 2.6.3 Diagnostics and Posterior mcmc_rank_overlay(mdl3) summary(mdl3) ## ## Model Info: ## function: stan_gamm4 ## family: gaussian [identity] ## formula: mpg ~ s(disp, bs = &quot;cr&quot;, k = 7) ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 32 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 20.1 0.4 19.5 20.1 20.7 ## s(disp).1 0.2 1.2 -1.1 0.1 1.6 ## s(disp).2 -0.9 1.1 -2.3 -0.7 0.3 ## s(disp).3 0.0 0.6 -0.6 0.0 0.7 ## s(disp).4 1.2 0.4 0.7 1.2 1.6 ## s(disp).5 0.4 0.1 0.2 0.4 0.6 ## s(disp).6 -3.1 0.3 -3.5 -3.1 -2.8 ## sigma 2.4 0.3 2.0 2.4 2.9 ## smooth_sd[s(disp)1] 1.2 0.7 0.5 1.0 2.0 ## smooth_sd[s(disp)2] 3.6 2.1 1.7 3.0 6.2 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 20.1 0.6 19.3 20.1 20.9 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3813 ## s(disp).1 0.0 1.0 3274 ## s(disp).2 0.0 1.0 2149 ## s(disp).3 0.0 1.0 3950 ## s(disp).4 0.0 1.0 2639 ## s(disp).5 0.0 1.0 4652 ## s(disp).6 0.0 1.0 3521 ## sigma 0.0 1.0 2017 ## smooth_sd[s(disp)1] 0.0 1.0 1215 ## smooth_sd[s(disp)2] 0.0 1.0 2055 ## mean_PPD 0.0 1.0 3909 ## log-posterior 0.1 1.0 953 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). The chains, n_eff and \\(\\widehat{R}\\) look good. In the Estimates section above, we also see the posteriors for the model parameters; there isn’t an intuitive interpretation of the spline coefficients so I’ll skip ahead to the posterior predictive distribution. 2.6.4 Posterior Predictive Distribution ppc_dens_overlay(mtcars$mpg, posterior_predict(mdl3, draws=50)) The expectation over the ppd is plotted below, along with a loess curve for comparison. This model is clearly a better fit to the data than the linear model. plot_nonlinear(mdl3, prob=0.95) + geom_point(mapping=aes(x=disp, y=mpg-mean(mpg)), data=mtcars) + labs(title=&quot;GAM&quot;, x=&quot;disp&quot;, y=&quot;mpg-mean(mpg)&quot;) ggplot(mapping=aes(x=disp, y=mpg-mean(mpg)), data=mtcars) + geom_point()+ stat_smooth(method=&quot;loess&quot;, level=0.95) + labs(title=&quot;LOESS&quot;) 2.7 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## other attached packages: ## [1] bayesplot_1.7.2 rstanarm_2.21.1 Rcpp_1.0.5 kableExtra_1.3.1 ## [5] gridExtra_2.3 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [9] purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 tibble_3.0.4 ## [13] ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] minqa_1.2.4 colorspace_2.0-0 ellipsis_0.3.1 ## [4] ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [7] base64enc_0.1-3 fs_1.5.0 rstudioapi_0.13 ## [10] farver_2.0.3 rstan_2.21.2 DT_0.16 ## [13] fansi_0.4.1 lubridate_1.7.9.2 xml2_1.3.2 ## [16] splines_4.0.3 codetools_0.2-16 knitr_1.30 ## [19] shinythemes_1.1.2 jsonlite_1.7.1 nloptr_1.2.2.2 ## [22] broom_0.7.2 dbplyr_2.0.0 shiny_1.5.0 ## [25] compiler_4.0.3 httr_1.4.2 backports_1.2.0 ## [28] Matrix_1.2-18 assertthat_0.2.1 fastmap_1.0.1 ## [31] cli_2.2.0 later_1.1.0.1 htmltools_0.5.0 ## [34] prettyunits_1.1.1 tools_4.0.3 igraph_1.2.6 ## [37] gtable_0.3.0 glue_1.4.2 reshape2_1.4.4 ## [40] V8_3.4.0 cellranger_1.1.0 vctrs_0.3.5 ## [43] nlme_3.1-149 crosstalk_1.1.0.1 xfun_0.19 ## [46] ps_1.4.0 lme4_1.1-26 rvest_0.3.6 ## [49] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [52] renv_0.12.0 gtools_3.8.2 statmod_1.4.35 ## [55] MASS_7.3-53 zoo_1.8-8 scales_1.1.1 ## [58] colourpicker_1.1.0 hms_0.5.3 promises_1.1.1 ## [61] parallel_4.0.3 inline_0.3.17 shinystan_2.5.0 ## [64] yaml_2.2.1 curl_4.3 loo_2.3.1 ## [67] StanHeaders_2.21.0-6 stringi_1.5.3 highr_0.8 ## [70] dygraphs_1.1.1.6 boot_1.3-25 pkgbuild_1.1.0 ## [73] rlang_0.4.9 pkgconfig_2.0.3 matrixStats_0.57.0 ## [76] evaluate_0.14 lattice_0.20-41 labeling_0.4.2 ## [79] rstantools_2.1.1 htmlwidgets_1.5.2 tidyselect_1.1.0 ## [82] processx_3.4.5 plyr_1.8.6 magrittr_2.0.1 ## [85] bookdown_0.21 R6_2.5.0 generics_0.1.0 ## [88] DBI_1.1.0 mgcv_1.8-33 pillar_1.4.7 ## [91] haven_2.3.1 withr_2.3.0 xts_0.12.1 ## [94] survival_3.2-7 modelr_0.1.8 crayon_1.3.4 ## [97] rmarkdown_2.5 grid_4.0.3 readxl_1.3.1 ## [100] callr_3.5.1 threejs_0.3.3 reprex_0.3.0 ## [103] digest_0.6.27 webshot_0.5.2 xtable_1.8-4 ## [106] httpuv_1.5.4 RcppParallel_5.0.2 stats4_4.0.3 ## [109] munsell_0.5.0 viridisLite_0.3.0 shinyjs_2.0.0 References "],["rethinking.html", "Chapter 3 rethinking 3.1 Resources 3.2 Description 3.3 Environment Setup 3.4 Linear Model 3.5 Semi-parametric Model 3.6 Session Info", " Chapter 3 rethinking 3.1 Resources Statistical Rethinking by McElreath Statistical Rethinking Lectures on YouTube rethinking github repo 3.2 Description Statistical Rethinking was one of the first books I read on Bayesian methods, and I highly recommend it. McElreath uses a lot of practical examples which I found very helpful. All of the problems in the book are done with the rethinking package which uses the familiar formula syntax for defining models. However, unlike rstanarm the functions are not close mirrors of familiar frequentist functions. Another difference from rstanarm is you must specify all priors–there are no defaults. The rethinking package has some nice extras. One is the stancode function which returns the stan code generated for the model. This is a great way to start getting familiar with stan syntax! Second map2stan returns an object that contains a stanfit object which you can access with the \\@stanfit accessor. Most of the bayesplot and shinystan functions work with that stanfit object. Alternatively, the rethinking package includes its own functions that work directly on the returned map2stan object (see the book for details). I ran into some difficulty with the semi-parametric regression (3.5), but aside from that the rethinking package is also a very good option for getting started. 3.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(datasets) data(mtcars) library(rethinking) library(bayesplot) 3.4 Linear Model 3.4.1 Define Model The rethinking package does not have default priors so I need to explicitly choose them. Again I’ll use the following model: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim Normal(13.2, 5.3^2) \\\\ b &amp;\\sim Normal(-0.1, 0.05^2) \\\\ \\sigma &amp;\\sim Exponential(1) \\end{align*}\\] # Note the sign change for mu and b, this seems to be a quirk # of map2stan that it didn&#39;t like b ~ dnorm(-0.1, 0.05) f &lt;- alist( mpg ~ dnorm(mu, sigma), mu &lt;- a - b * disp, a ~ dnorm(13.2, 5.3), b ~ dnorm(0.1, 0.05), sigma ~ dexp(1) ) # Note the default number of chains = 1, so I&#39;m explicitly setting to available cores mdl1 &lt;- map2stan(f,mtcars, chains=parallel::detectCores()) ## Trying to compile a simple C file ## Computing WAIC The automatically generated stan code: stancode(mdl1) ## //2020-12-15 20:39:53 ## data{ ## int&lt;lower=1&gt; N; ## real mpg[N]; ## real disp[N]; ## } ## parameters{ ## real a; ## real b; ## real&lt;lower=0&gt; sigma; ## } ## model{ ## vector[N] mu; ## sigma ~ exponential( 1 ); ## b ~ normal( 0.1 , 0.05 ); ## a ~ normal( 13.2 , 5.3 ); ## for ( i in 1:N ) { ## mu[i] = a - b * disp[i]; ## } ## mpg ~ normal( mu , sigma ); ## } ## generated quantities{ ## vector[N] mu; ## for ( i in 1:N ) { ## mu[i] = a - b * disp[i]; ## } ## } 3.4.2 Prior Predictive Distribution # Plot prior predictive distribution N &lt;- 50 prior_samples &lt;- as.data.frame(extract.prior(mdl1, n=N)) D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) res &lt;- as.data.frame(apply(prior_samples, 1, function(x) x[1] - x[2] * (D-mean(mtcars$disp)))) %&gt;% mutate(disp = D) %&gt;% pivot_longer(cols=c(-&quot;disp&quot;), names_to=&quot;iter&quot;) res %&gt;% ggplot() + geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) + labs(x=&quot;disp&quot;, y=&quot;prior predictive mpg&quot;) 3.4.3 Diagnostics mcmc_trace(mdl1@stanfit, pars=c(&quot;a&quot;, &quot;b&quot;, &quot;sigma&quot;)) mcmc_rank_overlay(mdl1@stanfit, pars=c(&quot;a&quot;, &quot;b&quot;, &quot;sigma&quot;)) The precis function is displays n_eff and \\(\\widehat{R}\\). precis(mdl1, prob=0.95) ## mean sd 2.5% 97.5% n_eff Rhat4 ## a 28.88390 1.179811 26.44239 31.14778 1545 1.002 ## b 0.03894 0.004514 0.02977 0.04761 1624 1.001 ## sigma 3.21343 0.403748 2.50514 4.12205 1769 1.001 3.4.4 Posterior Distribution The precis function above also displays both the posterior point estimate and credible interval. 3.4.5 Posterior Predictive Distribution Finally, I’ll check the posterior predictive distribution. The rethinking package includes the postcheck function which displays a plot for posterior predictive checking. postcheck(mdl1, window=nrow(mtcars)) Personally, I find the postcheck plot hard to use because I can never remember what the different symbols represent. Instead, I’ll plot the expectation of the posterior predictive distribution (i.e., \\(\\mu\\)) like I did with rstanarm . The sim function draws samples from the posterior predictive distribution, and the link function returns the linear predictor, possibly transformed by the inverse-link function. In this case, the model is a Gaussian likelihood with an identity link function, so the sim and link functions return identical results. newdata &lt;- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp))) y_rep &lt;- as.data.frame(t(link(mdl1, data=newdata, n=50))) %&gt;% cbind(newdata) %&gt;% pivot_longer(-disp, names_to=&quot;draw&quot;, values_to=&quot;mpg&quot;) y_rep %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_line(aes(group=draw), alpha=0.2) + geom_point(data = mtcars) 3.5 Semi-parametric Model 3.5.1 Define Model Setting up the semi-parametric model is a bit more work in the rethinking package. First, I explicitly create the splines. The component splines are plotted below. library(splines) num_knots &lt;- 4 # number of interior knots knot_list &lt;- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots)) B &lt;- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df1 &lt;- cbind(disp=mtcars$disp, B) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) # Plot at smaller intervals so curves are smooth N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) B_plot &lt;- bs(D, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df2 &lt;- cbind(disp=D, B_plot) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) ggplot(mapping=aes(x=disp, y=val, color=spline)) + geom_point(data=df1) + geom_line(data=df2, linetype=&quot;dashed&quot;) Then I define the model with the splines. I wasn’t able to get this model to work with either the map2stan or ulam functions, so I used quap instead which fits a quadratic approximation. f &lt;- alist( mpg ~ dnorm(mu, sigma), mu &lt;- a - B %*% w, a ~ dnorm(25, 10), w ~ dnorm(0,5), sigma ~ dexp(1) ) mdl2 &lt;- quap(f, data=list(mpg=mtcars$mpg, B=B), start=list(w=rep(1, ncol(B))) ) 3.5.2 Diagnostics Since MCMC was not used to fit the model, there are no chain diagnostics to examine. 3.5.3 Posterior Distribution I can still use the precis function to look at the posterior distribution, although there’s really no intuitive interpretation for the spline weights. precis(mdl2, depth=2) ## mean sd 5.5% 94.5% ## w[1] -12.0969 2.2958 -15.766 -8.4279 ## w[2] -4.3255 2.5531 -8.406 -0.2451 ## w[3] 0.4836 2.7405 -3.896 4.8636 ## w[4] 5.8888 2.8873 1.274 10.5034 ## w[5] 2.1741 2.8840 -2.435 6.7832 ## w[6] 9.0515 2.3915 5.229 12.8736 ## a 20.1951 2.0329 16.946 23.4440 ## sigma 1.9638 0.2397 1.581 2.3469 3.5.4 Posterior Predictive Distribution Finally, the posterior predictive distribution and LOESS for comparison: mu &lt;- link(mdl2) mu_mean &lt;- as.data.frame(apply(mu, 2, mean)) %&gt;% mutate(disp=mtcars$disp) colnames(mu_mean) &lt;- c(&quot;mpg_ppd&quot;, &quot;disp&quot;) mu_PI &lt;- as.data.frame(t(apply(mu,2,PI,0.95))) %&gt;% mutate(disp=mtcars$disp) colnames(mu_PI) &lt;- c(&quot;lwr&quot;, &quot;upr&quot;, &quot;disp&quot;) ggplot() + geom_point(data=mtcars, aes(x=disp, y=mpg)) + geom_line(data=mu_mean, aes(x=disp, y=mpg_ppd), color=&quot;blue&quot;) + geom_ribbon(data=mu_PI, aes(x=disp, ymin=lwr, ymax=upr), alpha=0.2) + labs(title=&quot;GAM&quot;) ggplot(mapping=aes(x=disp, y=mpg-mean(mpg)), data=mtcars) + geom_point()+ stat_smooth(method=&quot;loess&quot;, level=0.95) + labs(title=&quot;LOESS&quot;) 3.6 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] splines parallel stats graphics grDevices datasets utils ## [8] methods base ## ## other attached packages: ## [1] bayesplot_1.7.2 rethinking_2.13 rstan_2.21.2 ## [4] StanHeaders_2.21.0-6 forcats_0.5.0 stringr_1.4.0 ## [7] dplyr_1.0.2 purrr_0.3.4 readr_1.4.0 ## [10] tidyr_1.1.2 tibble_3.0.4 ggplot2_3.3.2 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-149 matrixStats_0.57.0 fs_1.5.0 lubridate_1.7.9.2 ## [5] httr_1.4.2 tools_4.0.3 backports_1.2.0 R6_2.5.0 ## [9] mgcv_1.8-33 DBI_1.1.0 colorspace_2.0-0 withr_2.3.0 ## [13] tidyselect_1.1.0 gridExtra_2.3 prettyunits_1.1.1 processx_3.4.5 ## [17] curl_4.3 compiler_4.0.3 cli_2.2.0 rvest_0.3.6 ## [21] xml2_1.3.2 labeling_0.4.2 bookdown_0.21 scales_1.1.1 ## [25] mvtnorm_1.1-1 ggridges_0.5.2 callr_3.5.1 digest_0.6.27 ## [29] rmarkdown_2.5 pkgconfig_2.0.3 htmltools_0.5.0 dbplyr_2.0.0 ## [33] rlang_0.4.9 readxl_1.3.1 rstudioapi_0.13 shape_1.4.5 ## [37] generics_0.1.0 farver_2.0.3 jsonlite_1.7.1 inline_0.3.17 ## [41] magrittr_2.0.1 loo_2.3.1 Matrix_1.2-18 Rcpp_1.0.5 ## [45] munsell_0.5.0 fansi_0.4.1 lifecycle_0.2.0 stringi_1.5.3 ## [49] yaml_2.2.1 MASS_7.3-53 pkgbuild_1.1.0 plyr_1.8.6 ## [53] grid_4.0.3 crayon_1.3.4 lattice_0.20-41 haven_2.3.1 ## [57] hms_0.5.3 knitr_1.30 ps_1.4.0 pillar_1.4.7 ## [61] reshape2_1.4.4 codetools_0.2-16 stats4_4.0.3 reprex_0.3.0 ## [65] glue_1.4.2 evaluate_0.14 V8_3.4.0 renv_0.12.0 ## [69] RcppParallel_5.0.2 modelr_0.1.8 vctrs_0.3.5 cellranger_1.1.0 ## [73] gtable_0.3.0 assertthat_0.2.1 xfun_0.19 broom_0.7.2 ## [77] coda_0.19-4 ellipsis_0.3.1 "],["rstan.html", "Chapter 4 rstan 4.1 Resources 4.2 Description 4.3 Environment Setup 4.4 Linear Model 4.5 Semi-parametric Model 4.6 Semi-parametric Model (Random Walk Prior) 4.7 Session Info", " Chapter 4 rstan 4.1 Resources mc-stan online documentation 4.2 Description All of the packages in the previous chapters are running stan under the hood. One of the biggest advantages to using rstan (or cmdstanr) is you get the full power and flexibility of stan so you can build models that aren’t supported by the other packages. The tradeoff is the syntax is quite different which can be a challenge for those who are only familiar with R. 4.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(datasets) data(mtcars) library(rstan) library(bayesplot) # Saves compiled version of model so it only has to be recompiled if the model is changed rstan_options(auto_write = TRUE) # Set number of cores options(mc.cores = parallel::detectCores()-1) 4.4 Linear Model 4.4.1 Define Model Like the rethinking package, rstan doesn’t have default priors, so I need to explicitly choose them: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim Normal(13.2, 5.3^2) \\\\ b &amp;\\sim Normal(-0.1, 0.05^2) \\\\ \\sigma &amp;\\sim Exponential(1) \\end{align*}\\] For a simple linear model there are three sections to the model definition: data - This is where the data structures for the known/observed portions of the model (e.g., the number of observations, the number and type of predictors) are defined. parameters - This is where the data structures for the parameters to be estimated are defined. For example, the coefficients of the simple linear model belong in this section. model - This is where the model (including priors) is defined using the data structures from the previous sections. # Define model mdl_code &lt;- &#39; data{ int&lt;lower=1&gt; N; vector[N] mpg; vector[N] disp; } parameters{ real a; real b; real&lt;lower=0.0&gt; sigma; } model{ // Likelihood mpg ~ normal(a + b * disp, sigma); // Priors a ~ normal(13.2, 5.3); b ~ normal(-0.1, 0.05); sigma ~ exponential(1); } &#39; A couple of comments about the model definition. For those only familiar with R, it may seem like a lot of extra “stuff” is going on in the data and parameters sections. This is because stan is written in C++ which is statically typed, unlike R and Python which are dynamically typed. Essentially what that means is you must define the type of any variable before you use it. The lower= (and upper= not shown in this example) options define bounds for a variable. The data is checked against the bounds which can detect errors pre-compilation. Generally, bounds are a good idea but aren’t required. Next, populate the data structures from the data section and save in a list. mdl_data &lt;- list(N = nrow(mtcars), mpg = mtcars$mpg, disp = mtcars$disp) And this is the call to fit the model. # Fit model mdl1 &lt;- stan(model_code=mdl_code, data=mdl_data, model_name=&quot;mdl1&quot;) ## Trying to compile a simple C file 4.4.2 Prior Predictive Distribution I could manually construct the prior predictive distribution like I did in 3.4.2. Instead I’ll have stan generate the prior predictive distribution which will be useful for more complex models. First, create another model with just the data and generated quantities section. The generated quantities section mirrors the model section except it is now drawing samples from the priors without conditioning on the observed data. Also, in the stan call set the sampling algorithm for fixed parameters. # Plot prior predictive distribution mdl_prior &lt;- &#39; data{ int&lt;lower=1&gt; N; vector[N] disp; } generated quantities{ real a_sim = normal_rng(13.2, 5.3); real b_sim = normal_rng(-0.1, 0.05); real sigma_sim = exponential_rng(1); real mpg_sim[N] = normal_rng(a_sim + b_sim * disp, sigma_sim); } &#39; N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) mdl_data_prior &lt;- list(N = N, disp=D) mdl_prior &lt;- stan(model_code=mdl_prior, data=mdl_data_prior, model_name=&quot;mdl_prior&quot;, chains=1, algorithm=&quot;Fixed_param&quot;) ## Trying to compile a simple C file draws &lt;- as.data.frame(mdl_prior) %&gt;% head(50) # Expected value prior predictive distribution exp_mpg_sim &lt;- apply(draws, 1, function(x) x[&quot;a_sim&quot;] + x[&quot;b_sim&quot;] * (D-mean(mtcars$disp))) %&gt;% as.data.frame() %&gt;% mutate(disp = D) %&gt;% pivot_longer(-c(&quot;disp&quot;), names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) ggplot() + geom_line(data=exp_mpg_sim, mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) 4.4.3 Diagnostics mcmc_rank_overlay(mdl1, pars=c(&quot;a&quot;, &quot;b&quot;, &quot;sigma&quot;)) print(mdl1) ## Inference for Stan model: mdl1. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a 28.85 0.03 1.19 26.47 28.05 28.84 29.64 31.22 1681 1 ## b -0.04 0.00 0.00 -0.05 -0.04 -0.04 -0.04 -0.03 1743 1 ## sigma 3.21 0.01 0.40 2.56 2.93 3.18 3.45 4.14 1789 1 ## lp__ -61.51 0.04 1.25 -64.85 -62.06 -61.18 -60.60 -60.11 1190 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Dec 15 20:41:10 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 4.4.4 Posterior Distribution The print function above displays information about the posterior distributions in addition to n_eff. Alternatively, the plot function provides a graphical display of the posterior distributions. plot(mdl1, ci_level=0.89) ## ci_level: 0.89 (89% intervals) ## outer_level: 0.95 (95% intervals) 4.4.5 Posterior Predictive Distribution Using the posterior samples, I can plot the expected value of the posterior predictive distribution. N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) draws &lt;- as.data.frame(mdl1) %&gt;% head(50) # Expected value posterior predictive distribution post_pred &lt;- apply(draws, 1, function(x) x[&quot;a&quot;] + x[&quot;b&quot;]*(D)) %&gt;% as.data.frame() %&gt;% mutate(disp = D) %&gt;% pivot_longer(-c(&quot;disp&quot;), names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) ggplot() + geom_line(data=post_pred, mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg)) Note that the expected value of the ppd doesn’t include \\(\\sigma\\). An alternative is to have stan automatically generate samples from the posterior predictive distribution by adding a generated quantities section to the model (similar to what I did for the prior predictive distribution). # Define model mdl_code_ppd &lt;- &#39; data{ int&lt;lower=1&gt; N; vector[N] mpg; vector[N] disp; } parameters{ real a; real b; real&lt;lower=0.0&gt; sigma; } transformed parameters{ // Expected value of posterior predictive vector[N] Y_hat = a + b * disp; } model{ // Likelihood mpg ~ normal(Y_hat, sigma); // Priors a ~ normal(13.2, 5.3); b ~ normal(-0.1, 0.05); sigma ~ exponential(1); } generated quantities{ // Posterior Predictive real mpg_ppd[N] = normal_rng(Y_hat, sigma); } &#39; # Fit model mdl1_ppd &lt;- stan(model_code=mdl_code_ppd, data=mdl_data) ## Trying to compile a simple C file draws &lt;- as.data.frame(mdl1_ppd) # 95% credible interval for expected value of ppd Eppd &lt;- draws %&gt;% select(starts_with(&quot;Y_hat&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) # 95 credible interval for ppd ppd &lt;- draws %&gt;% select(starts_with(&quot;mpg_ppd&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp=mdl_data$disp) ggplot() + geom_line(data=Eppd, mapping=aes(x=disp, y=`50%`)) + geom_ribbon(data=ppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;lightblue&quot;) + geom_ribbon(data=Eppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;dodgerblue&quot;) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg)) + labs(x=&quot;disp&quot;, y=&quot;mpg&quot;) The darker blue area is a 95% credible interval for the expected value of the posterior predictive distribution and the lighter blue area is the 95% credible interval for the posterior predictive distribution. And we can also plot the density overlay using the posterior predictive draws. yrep &lt;- draws %&gt;% head(50) %&gt;% select(starts_with(&quot;mpg_ppd&quot;)) %&gt;% as.matrix() ppc_dens_overlay(mtcars$mpg, yrep) 4.5 Semi-parametric Model 4.5.1 Define Model First, I’ll define the splines just as I did with the rethinking package. library(splines) num_knots &lt;- 4 # number of interior knots knot_list &lt;- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots)) B &lt;- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df1 &lt;- cbind(disp=mtcars$disp, B) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) # Plot at smaller intervals so curves are smooth N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) B_plot &lt;- bs(D, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df2 &lt;- cbind(disp=D, B_plot) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) ggplot(mapping=aes(x=disp, y=val, color=spline)) + geom_point(data=df1) + geom_line(data=df2, linetype=&quot;dashed&quot;) Note: the dashed lines are the splines and the points are the values of the spline at the specific values of mtcars$disp; the points are inputs into the stan model. # Define model mdl_code &lt;- &#39; data{ int&lt;lower=1&gt; N; int&lt;lower=1&gt; num_basis; vector[N] mpg; vector[N] disp; matrix[N, num_basis] B; } parameters{ real a; real&lt;lower=0.0&gt; sigma; vector[num_basis] w; } transformed parameters{ vector[N] Y_hat = a + B*w; } model{ // Likelihood mpg ~ normal(Y_hat, sigma); // Priors a ~ normal(25, 10); sigma ~ exponential(1); w ~ normal(0, 5); } generated quantities{ // Posterior Predictive real mpg_ppd[N] = normal_rng(Y_hat, sigma); } &#39; mdl_data &lt;- list(N=nrow(mtcars), num_basis=ncol(B), B=B, mpg = mtcars$mpg, disp = mtcars$disp) # Fit model mdl1_gam &lt;- stan(model_code=mdl_code, data=mdl_data) ## Trying to compile a simple C file 4.5.2 Prior Predictive Distribution # Define model mdl2 &lt;- &#39; data{ int&lt;lower=1&gt; N; int&lt;lower=1&gt; num_basis; //vector[N] mpg; //vector[N] disp; matrix[N, num_basis] B; } generated quantities{ real a_sim = normal_rng(25, 10); real sigma_sim = exponential_rng(1); real mpg_sim[N]; vector[N] Y_hat; vector[num_basis] w_sim; for (i in 1:num_basis) w_sim[i] = normal_rng(0,5); Y_hat = a_sim + B * w_sim; mpg_sim = normal_rng(Y_hat, sigma_sim); } &#39; mdl_gam_prior &lt;- stan(model_code=mdl2, data=list(N=N, num_basis=ncol(B_plot), B=B_plot), chains=1, algorithm=&quot;Fixed_param&quot;) ## Trying to compile a simple C file draws &lt;- as.data.frame(mdl_gam_prior) %&gt;% head(50) # Expected value prior predictive distribution exp_mpg_sim &lt;- apply(draws, 1, function(x) { x[&quot;a_sim&quot;] + B_plot %*% x[grepl(&quot;w&quot;, names(x))] }) %&gt;% as.data.frame() %&gt;% mutate(disp = D) %&gt;% pivot_longer(-c(&quot;disp&quot;), names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) # 95% interval prior predictive distribution mpg_sim &lt;- as.data.frame(mdl_gam_prior) %&gt;% select(starts_with(&quot;mpg&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = D) ggplot() + geom_line(data=exp_mpg_sim, mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) + geom_ribbon(data=mpg_sim, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;lightblue&quot;) 4.5.3 Diagnostics # Note that bayesplot methods support tidy selection of parameters mcmc_rank_overlay(mdl1_gam, pars=vars(a, sigma, starts_with(&quot;w&quot;))) # This is the print.stanfit method and pars must be a character vector print(mdl1_gam, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) ## Inference for Stan model: 82641c24a01364d6c7401a2add0dc8c4. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a 20.30 0.06 2.06 16.49 18.88 20.28 21.70 24.31 1063 1 ## sigma 2.23 0.01 0.32 1.71 2.01 2.20 2.42 2.98 1929 1 ## w[1] 11.77 0.07 2.42 7.13 10.13 11.78 13.42 16.43 1210 1 ## w[2] 4.25 0.07 2.66 -1.11 2.49 4.28 6.07 9.47 1603 1 ## w[3] -0.69 0.08 2.85 -6.23 -2.64 -0.66 1.26 4.79 1372 1 ## w[4] -5.78 0.07 3.00 -11.67 -7.75 -5.80 -3.80 0.18 1618 1 ## w[5] -2.47 0.08 3.03 -8.47 -4.52 -2.41 -0.46 3.59 1536 1 ## w[6] -8.91 0.07 2.53 -13.84 -10.64 -8.88 -7.11 -4.02 1487 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Dec 15 20:42:48 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 4.5.4 Posterior Distribution plot(mdl1_gam, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) 4.5.5 Posterior Predictive Distribution # 95% credible interval expected value of posterior predictive Eppd &lt;- as.data.frame(mdl1_gam) %&gt;% select(starts_with(&quot;Y_hat&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) # 95% credible interval posterior predictive ppd &lt;- as.data.frame(mdl1_gam) %&gt;% select(starts_with(&quot;mpg&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) ggplot() + geom_line(data=Eppd, mapping=aes(x=disp, y=`50%`)) + geom_ribbon(data=ppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;lightblue&quot;) + geom_ribbon(data=Eppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;dodgerblue&quot;) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg)) + labs(x=&quot;disp&quot;, y=&quot;mpg&quot;) 4.6 Semi-parametric Model (Random Walk Prior) 4.6.1 Define Model One challenge with splines is choosing the number of knots. For the previous model, I tried several values for num_knots until settling on 4. However, there is a stan case study for splines that uses a novel prior which addresses this issue. The details are here. For this example, I will set num_knots=20 and then fit models with and without the random walk prior. This is an example where the rstan’s flexibility is an advantage because it would be difficult or impossible to specify the random walk prior in the other packages. library(splines) num_knots &lt;- 20 # number of interior knots knot_list &lt;- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots)) B &lt;- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE) # Define model with smoothing prior mdl_smooth_code &lt;- &#39; data{ int&lt;lower=1&gt; N; int&lt;lower=1&gt; num_basis; vector[N] mpg; vector[N] disp; matrix[N, num_basis] B; } parameters{ real a; real&lt;lower=0.0&gt; sigma; vector[num_basis] w_raw; real&lt;lower=0.0&gt; tau; } transformed parameters{ vector[num_basis] w; vector[N] Y_hat; w[1] = w_raw[1]; for (i in 2:num_basis) w[i] = w[i-1] + w_raw[i]*tau; Y_hat = a + B*w; } model{ // Likelihood mpg ~ normal(Y_hat, sigma); // Priors a ~ normal(25, 10); sigma ~ exponential(1); w_raw ~ normal(0, 1); tau ~ normal(0,1); } generated quantities{ real mpg_ppd[N] = normal_rng(a + B*w, sigma); } &#39; mdl_data &lt;- list(N=nrow(mtcars), num_basis=ncol(B), B=B, mpg = mtcars$mpg, disp = mtcars$disp) # Fit model with smoothing prior mdl2_gam_smooth &lt;- stan(model_code=mdl_smooth_code, data=mdl_data, control=list(adapt_delta=0.99)) ## Trying to compile a simple C file # Fit model without smoothing prior mdl2_gam &lt;- stan(model_code = mdl_code, data=mdl_data, control=list(adapt_delta=0.99)) 4.6.2 Diagnostics mcmc_rank_overlay(mdl2_gam_smooth, pars=vars(&quot;a&quot;, &quot;sigma&quot;, starts_with(&quot;w&quot;))) print(mdl2_gam_smooth, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) ## Inference for Stan model: 38c293991ee2b9bb0106a5b740561b9f. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a 32.01 0.04 2.02 28.08 30.69 32.04 33.36 35.97 3196 1 ## sigma 2.17 0.01 0.32 1.64 1.95 2.13 2.37 2.90 2955 1 ## w[1] 0.06 0.01 1.01 -1.89 -0.60 0.06 0.72 2.00 4659 1 ## w[2] -1.07 0.03 1.93 -4.92 -2.33 -1.03 0.20 2.65 3383 1 ## w[3] -2.20 0.04 2.14 -6.49 -3.65 -2.20 -0.81 2.04 3624 1 ## w[4] -3.52 0.04 2.36 -8.15 -5.11 -3.57 -1.97 1.08 3938 1 ## w[5] -5.96 0.04 2.47 -10.99 -7.56 -5.92 -4.27 -1.35 3354 1 ## w[6] -8.29 0.04 2.37 -12.92 -9.82 -8.31 -6.71 -3.69 3078 1 ## w[7] -9.18 0.04 2.50 -14.17 -10.84 -9.18 -7.46 -4.38 3507 1 ## w[8] -9.75 0.04 2.41 -14.48 -11.33 -9.77 -8.14 -5.01 3846 1 ## w[9] -10.53 0.04 2.43 -15.40 -12.09 -10.52 -8.93 -5.75 3839 1 ## w[10] -11.98 0.04 2.27 -16.47 -13.46 -11.99 -10.45 -7.47 3228 1 ## w[11] -12.99 0.05 2.57 -18.13 -14.63 -12.97 -11.24 -8.06 3187 1 ## w[12] -13.31 0.04 2.57 -18.42 -14.98 -13.29 -11.64 -8.22 3449 1 ## w[13] -13.84 0.04 2.38 -18.49 -15.38 -13.82 -12.31 -9.16 3797 1 ## w[14] -15.60 0.04 2.42 -20.37 -17.17 -15.59 -14.01 -10.84 2957 1 ## w[15] -16.38 0.05 2.44 -21.20 -17.98 -16.40 -14.74 -11.46 2802 1 ## w[16] -16.61 0.04 2.52 -21.62 -18.30 -16.61 -14.91 -11.71 3392 1 ## w[17] -16.68 0.04 2.42 -21.39 -18.29 -16.66 -15.06 -12.05 3295 1 ## w[18] -15.90 0.04 2.31 -20.49 -17.40 -15.93 -14.37 -11.36 3491 1 ## w[19] -15.83 0.04 2.47 -20.86 -17.40 -15.80 -14.24 -10.98 4198 1 ## w[20] -17.44 0.04 2.52 -22.56 -19.10 -17.44 -15.75 -12.64 4156 1 ## w[21] -19.55 0.05 2.64 -24.68 -21.32 -19.56 -17.77 -14.40 3178 1 ## w[22] -20.51 0.05 2.75 -25.90 -22.40 -20.52 -18.65 -15.02 3126 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Dec 15 20:43:58 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 4.6.3 Posterior Distribution plot(mdl2_gam_smooth, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) plot(mdl2_gam, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) 4.6.4 Posterior Predictive Distribution draws &lt;- as.data.frame(mdl2_gam) %&gt;% head(100) Epost_pred &lt;- draws %&gt;% select(starts_with(&quot;Y_hat&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.055, 0.5, 0.945))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) draws_smooth &lt;- as.data.frame(mdl2_gam_smooth) %&gt;% head(100) Epost_pred_smooth &lt;- draws_smooth %&gt;% select(starts_with(&quot;Y_hat&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.055, 0.5, 0.945))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) rbind(Epost_pred %&gt;% select(c(&quot;disp&quot;, `50%`)) %&gt;% mutate(type=&quot;without smoothing prior&quot;), Epost_pred_smooth %&gt;% select(c(&quot;disp&quot;, `50%`)) %&gt;% mutate(type=&quot;with smoothing prior&quot;)) %&gt;% ggplot() + geom_line( mapping=aes(x=disp, y=`50%`, linetype=type), color=&quot;blue&quot; ) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg)) + labs(y=&quot;mpg&quot;) The plot above shows that even with a large number of knots (in this case 20), the model with the smoothing prior significantly reduces over-fitting when compared to the model without the smoothing prior. 4.7 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] splines stats graphics grDevices datasets utils methods ## [8] base ## ## other attached packages: ## [1] bayesplot_1.7.2 rstan_2.21.2 StanHeaders_2.21.0-6 ## [4] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [7] purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 ## [10] tibble_3.0.4 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 jsonlite_1.7.1 modelr_0.1.8 RcppParallel_5.0.2 ## [5] assertthat_0.2.1 stats4_4.0.3 renv_0.12.0 cellranger_1.1.0 ## [9] yaml_2.2.1 pillar_1.4.7 backports_1.2.0 glue_1.4.2 ## [13] digest_0.6.27 rvest_0.3.6 colorspace_2.0-0 htmltools_0.5.0 ## [17] plyr_1.8.6 pkgconfig_2.0.3 broom_0.7.2 haven_2.3.1 ## [21] bookdown_0.21 scales_1.1.1 processx_3.4.5 farver_2.0.3 ## [25] generics_0.1.0 ellipsis_0.3.1 withr_2.3.0 cli_2.2.0 ## [29] magrittr_2.0.1 crayon_1.3.4 readxl_1.3.1 evaluate_0.14 ## [33] ps_1.4.0 fs_1.5.0 fansi_0.4.1 xml2_1.3.2 ## [37] pkgbuild_1.1.0 tools_4.0.3 loo_2.3.1 prettyunits_1.1.1 ## [41] hms_0.5.3 lifecycle_0.2.0 matrixStats_0.57.0 V8_3.4.0 ## [45] munsell_0.5.0 reprex_0.3.0 callr_3.5.1 compiler_4.0.3 ## [49] rlang_0.4.9 grid_4.0.3 ggridges_0.5.2 rstudioapi_0.13 ## [53] labeling_0.4.2 rmarkdown_2.5 gtable_0.3.0 codetools_0.2-16 ## [57] inline_0.3.17 DBI_1.1.0 curl_4.3 reshape2_1.4.4 ## [61] R6_2.5.0 gridExtra_2.3 lubridate_1.7.9.2 knitr_1.30 ## [65] stringi_1.5.3 parallel_4.0.3 Rcpp_1.0.5 vctrs_0.3.5 ## [69] dbplyr_2.0.0 tidyselect_1.1.0 xfun_0.19 "],["references.html", "References", " References "]]
