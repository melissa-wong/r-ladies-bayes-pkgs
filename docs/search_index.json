[["index.html", "Intro to R Bayes Packages Chapter 1 Prerequisites", " Intro to R Bayes Packages Melissa Wong 2020-12-08 Chapter 1 Prerequisites "],["intro.html", "Chapter 2 Introduction 2.1 Resources 2.2 A Motivating Example 2.3 Workflow 2.4 Data", " Chapter 2 Introduction 2.1 Resources Bayesian Data Analysis by Andrew Gelman, et. al. A First Course in Bayesian Statistical Methods by Peter Hoff Statistical Rethinking by Richard McElreath 2.2 A Motivating Example Let’s start with something familiar–the Monty Hall problem. There are three doors labeled A, B and C. A car is behind one of the doors and a goat is behind each of the other two doors. You choose a door (let’s say A). Monty Hall, who knows where the car actually is, opens one of the other doors (let’s say B) revealing a goat. Do you stay with door A or do you switch to door C? We can frame the problem as follows: Initially, you believe the car is equally likely to behind each door (i.e., \\(P[A=car]=P[B=car]=P[C=car]=\\frac{1}{3}\\)). Let’s call this the prior information. Next, you can calculate the conditional probabilities that Monty Hall opened door B. Let’s call this the likelihood. \\[\\begin{align*} P[B=open | A=car] &amp;= \\frac{1}{2}\\\\ P[B=open | B=car] &amp;= 0 \\\\ P[B=open | C=car] &amp;=1 \\end{align*}\\] Finally, you can update your beliefs with the new information. Let’s call your updated beliefs the posterior. \\[\\begin{align*} P[A=car|B=open] &amp;= \\frac{P[B=open|A=car]P[A=car]}{P[B=open]} = \\frac{1/2 * 1/3}{1/6 + 0 + 1/3} = \\frac{1}{3} \\\\ P[B=car|B=open] &amp;= \\frac{P[B=open|B=car]P[B=car]}{P[B=open]} = \\frac{0 * 1/3}{1/6 + 0 + 1/3} = 0 \\\\ P[C=car|B=open] &amp;= \\frac{P[B=open|C=car]P[C=car]}{P[B=open]} = \\frac{1 * 1/3}{1/6 + 0 + 1/3} = \\frac{2}{3} \\end{align*}\\] Clearly, you should switch to door C. This is a toy illustration of how to think about a model in a Bayesian framework: \\[posterior \\propto likelihood * prior\\] (See the resources for a proper mathematical derivation.) 2.3 Workflow The workflow I’ll follow in the subsequent chapters is as follows: Define the model. Examine the prior predictive distribution. Examine diagnostic plots. Examine posterior distribution. Examine the posterior predictive distribution. In general, this is an iterative process. At each step you may discover something that causes you to start over at step 1 with a new, refined model. 2.4 Data For all of the examples, I use the mtcars data set and a model with disp as the predictor and mpg as the response. I start with a simple linear regression. However, as you can see from the scatterplot below, the relationship between mpg and disp is not linear, so I also fit a slightly more complex semi-parametric model. library(tidyverse) library(datasets) data(mtcars) mtcars %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_point() ## Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## other attached packages: ## [1] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 ## [5] readr_1.4.0 tidyr_1.1.2 tibble_3.0.4 ggplot2_3.3.2 ## [9] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.0 xfun_0.19 haven_2.3.1 colorspace_2.0-0 ## [5] vctrs_0.3.5 generics_0.1.0 htmltools_0.5.0 yaml_2.2.1 ## [9] rlang_0.4.9 pillar_1.4.7 glue_1.4.2 withr_2.3.0 ## [13] DBI_1.1.0 dbplyr_2.0.0 modelr_0.1.8 readxl_1.3.1 ## [17] lifecycle_0.2.0 munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 ## [21] rvest_0.3.6 evaluate_0.14 labeling_0.4.2 knitr_1.30 ## [25] fansi_0.4.1 broom_0.7.2 Rcpp_1.0.5 renv_0.12.0 ## [29] scales_1.1.1 backports_1.2.0 jsonlite_1.7.1 farver_2.0.3 ## [33] fs_1.5.0 hms_0.5.3 digest_0.6.27 stringi_1.5.3 ## [37] bookdown_0.21 grid_4.0.3 cli_2.2.0 tools_4.0.3 ## [41] magrittr_2.0.1 crayon_1.3.4 pkgconfig_2.0.3 ellipsis_0.3.1 ## [45] xml2_1.3.2 reprex_0.3.0 lubridate_1.7.9.2 assertthat_0.2.1 ## [49] rmarkdown_2.5 httr_1.4.2 rstudioapi_0.13 R6_2.5.0 ## [53] compiler_4.0.3 "],["rstanarm.html", "Chapter 3 rstanarm 3.1 Resources 3.2 Description 3.3 Environment Setup 3.4 Linear Model (Default Priors) 3.5 Linear Model (User-Defined Priors) 3.6 Semi-parametric Model 3.7 Session Info", " Chapter 3 rstanarm 3.1 Resources Regression and Other Stories by Gelman, Hill and Vehtari rstanarm online documentation 3.2 Description The rstanarm package is one of the easiest ways to get started with Bayesian models. The functions parallel the frequentist functions you’re probably already familiar with, and the syntax will also be familiar. You aren’t required to explicitly choose priors because all of the functions have weakly informative priors by default (although some might argue not being required to specify priors is a drawback). The primary limitation I’ve found thus far is the supported types for user-defined priors is somewhat limited. 3.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(gridExtra) library(kableExtra) library(datasets) data(mtcars) library(rstanarm) library(bayesplot) 3.4 Linear Model (Default Priors) 3.4.1 Define Model Let’s start with the following simple linear model: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ \\end{align*}\\] The stan_glm function from the rstanarm package fits a Bayesian linear model. The syntax is very similar to lm/glm. mdl1 &lt;- stan_glm(mpg ~ disp, data = mtcars) 3.4.2 Prior Predictive Distribution Next, I’ll examine the prior predictive distribution to see if the default priors seem reasonable. The prior_summary function shows the default priors for the model as well as the adjusted priors after automatic scaling. See http://mc-stan.org/rstanarm/articles/priors.html if you are interested in the details about how the default and adjusted priors are calculated. prior_summary(mdl1) ## Priors for model &#39;mdl1&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 20, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 20, scale = 15) ## ## Coefficients ## Specified prior: ## ~ normal(location = 0, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0, scale = 0.12) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.17) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details # Plot prior predictive distribution using adjusted priors N &lt;- 100 prior_samples &lt;- data.frame(a = rnorm(N, 20, 15), b = rnorm(N, 0, 0.12)) D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) res &lt;- as.data.frame(apply(prior_samples, 1, function(x) x[1] + x[2] * (D-mean(mtcars$disp)))) %&gt;% mutate(disp = D) %&gt;% pivot_longer(cols=c(-&quot;disp&quot;), names_to=&quot;iter&quot;) res %&gt;% ggplot() + geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) + labs(x=&quot;disp&quot;, y=&quot;prior predictive mpg&quot;) I notice two things in the prior predictive distribution which seem unrealistic given what I know about the real world: 1) negative mpg and 2) increasing mpg as displacement increases. Later on I’ll choose a more informative prior which incorporates this external knowledge. But let’s proceed with the analysis and see what happens. 3.4.3 Diagnostics I’ll walk through the steps for manually extracting the key diagnostic information from the mdl1 object since I think that can be helpful to understand exactly what’s going on. However, once you have a handle on these steps I highly recommend the shinystan package; it will automatically create all of these diagnostic plots (and more) with an nice interactive web interface. 3.4.3.1 Trace Plots The bayesplot package provides the function mcmc_trace which plots the Markov Chain Monte Carlo (MCMC) draws. mcmc_trace(mdl1, pars=c(&quot;disp&quot;, &quot;sigma&quot;)) There are three things I am looking for in the trace plot of each chain: Good mixing - In other words, the chain is rapidly changing values across the full region versus getting “stuck” near a particular value and slowly changing. Stationarity - The mean of the chain is relatively stable. Convergence - All of the chains spend most of the time around the same high-probability value. The trace plots above look good. 3.4.3.2 Trace Rank Plots It can sometimes be hard to interpret the trace plots when there are many chains. An alternative is the mcmc_rank_overlay function. This function plots a trace rank plot which is the distribution of the ranked samples; if the four chains have a roughly uniform distribution that indicates good mixing. mcmc_rank_overlay(mdl1, pars=c(&quot;disp&quot;, &quot;sigma&quot;)) 3.4.3.3 \\(\\widehat{R}\\) and Effective Sample Size In addition to visually examining the chains, we should also check \\(\\widehat{R}\\) which is a measure of convergence. \\(\\widehat{R} &gt; 1.0\\) indicates poor mixing, and the mc_stan documentation recommends only using samples if \\(\\widehat{R} &lt; 1.05\\). However, a recent paper by Vehtari et al. (2020) recommends \\(\\widehat{R} &lt; 1.01\\). Since MCMC samples are usually correlated, the effective sample size (n_eff) is often less than the number of samples. There is no hard and fast rule for what is an acceptable number for n_eff. McElreath’s guidance is it depends on what you are trying to estimate. If you are interested mostly in the posterior mean, then n_eff = 200 can be enough. But if you are interested in the tails of the distribution and it’s highly skewed then you’ll need n_eff to be much larger. There are two parameters, iter and warmup, which you can adjust in stan_glm if a larger n_eff is needed. The summary function displays n_eff and \\(\\widehat{R}\\) for the object returned by stan_glm. summary(mdl1) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: mpg ~ disp ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 32 ## predictors: 2 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 29.6 1.3 27.9 29.6 31.2 ## disp 0.0 0.0 0.0 0.0 0.0 ## sigma 3.4 0.5 2.8 3.3 4.0 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 20.1 0.9 19.0 20.1 21.2 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3501 ## disp 0.0 1.0 3328 ## sigma 0.0 1.0 3056 ## mean_PPD 0.0 1.0 3411 ## log-posterior 0.0 1.0 1529 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). 3.4.4 Posterior Distribution Since the chains, n_eff and \\(\\widehat{R}\\) look good, let’s examine the posterior distributions next. # Posterior point estimates (medians are used for point estimates) coef(mdl1) ## (Intercept) disp ## 29.57206 -0.04103 # 95% credible intervals knitr::kable(posterior_interval(mdl1, prob=0.95)) 2.5% 97.5% (Intercept) 27.0319 32.1255 disp -0.0509 -0.0313 sigma 2.6281 4.3876 3.4.5 Posterior Predictive Distribution The posterior_predict function draws samples from the posterior predictive distribution. The shinystan package also generates nice plots of the posterior predictive distribution. In this case, I’m going to plot the expected value of the posterior predictive distribution and overlay the observations since I think it can be easier to understand for someone new to these methods. The posterior_linpred function returns the linear predictor, possibly transformed by the inverse-link function. The posterior_epred function returns the expectation over the posterior predictive distribution. In this example, the model is a Gaussian likelihood with an identity link function, so the two functions return identical results. newdata &lt;- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp))) y_rep &lt;- as.data.frame(t(posterior_epred(mdl1, newdata=newdata, draws=50))) %&gt;% cbind(newdata) %&gt;% pivot_longer(cols=starts_with(&quot;V&quot;), names_to=&quot;grp&quot;, values_to=&quot;mpg&quot;) y_rep %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_line(aes(group=grp), alpha=0.2) + geom_point(data = mtcars) 3.5 Linear Model (User-Defined Priors) Now I’ll specify priors which incorporate my prior knowledge that mpg is non-negative and is non-increasing as disp increases. My new model is as follows: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim Normal(25,10^2) \\\\ b &amp;\\sim Normal(-0.2, 0.1^2) \\\\ \\sigma &amp;\\sim Exponential(1) \\end{align*}\\] The differences from the default priors are The intercept prior has a slightly smaller variance so negative values ofmpg are less likely. The slope prior is no longer symmetric about 0, but rather it is centered at -0.2 so that positive values are less likely. 3.5.1 Define Model mdl2 &lt;- stan_glm(mpg ~ disp, data = mtcars, prior = normal(-0.2, 0.1), # prior for slope prior_intercept = normal(25,10), # prior for intercept prior_aux = exponential(1)) # prior for standard deviation 3.5.2 Prior Predictive Distribution # Plot prior predictive distribution using adjusted priors N &lt;- 100 prior_samples &lt;- data.frame(a = rnorm(N, 25, 10), b = rnorm(N, -0.2, 0.1)) D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) res &lt;- as.data.frame(apply(prior_samples, 1, function(x) x[1] + x[2] * (D-mean(mtcars$disp)))) %&gt;% mutate(disp = D) %&gt;% pivot_longer(cols=c(-&quot;disp&quot;), names_to=&quot;iter&quot;) res %&gt;% ggplot() + geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) + labs(x=&quot;disp&quot;, y=&quot;prior predictive mpg&quot;) Compared to the prior predictive distribution with the default priors, this looks more consistent with my real-world knowledge that was reflected in the priors. 3.5.3 Diagnostics mcmc_trace(mdl2, pars=c(&quot;disp&quot;, &quot;sigma&quot;)) summary(mdl2) ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: mpg ~ disp ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 32 ## predictors: 2 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 29.7 1.2 28.1 29.7 31.2 ## disp 0.0 0.0 0.0 0.0 0.0 ## sigma 3.2 0.4 2.7 3.2 3.8 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 20.1 0.8 19.1 20.1 21.1 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3490 ## disp 0.0 1.0 3554 ## sigma 0.0 1.0 2908 ## mean_PPD 0.0 1.0 3469 ## log-posterior 0.0 1.0 1665 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). The trace plots, n_eff and \\(\\widehat{R}\\) all look good. 3.5.4 Posterior Distribution Now let’s compare the posterior with informative versus default priors: # Point estimates knitr::kable(cbind(coef(mdl1), coef(mdl2)), col.names = c(&quot;Default&quot;, &quot;Informative&quot;)) Default Informative (Intercept) 29.572 29.6599 disp -0.041 -0.0414 # 95% credible intervals knitr::kable(cbind(posterior_interval(mdl1, prob=0.95), posterior_interval(mdl2, prob=0.95))) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Default&quot; = 2, &quot;Informative&quot; = 2)) Default Informative 2.5% 97.5% 2.5% 97.5% (Intercept) 27.0319 32.1255 27.2755 32.059 disp -0.0509 -0.0313 -0.0509 -0.032 sigma 2.6281 4.3876 2.5155 4.151 In this case, there is sufficient data that the choice of prior really didn’t make much of a difference. 3.5.5 Posterior Predictive Distribution newdata &lt;- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp))) y_rep &lt;- as.data.frame(t(posterior_epred(mdl2, newdata=newdata, draws=50))) %&gt;% cbind(newdata) %&gt;% pivot_longer(cols=starts_with(&quot;V&quot;), names_to=&quot;grp&quot;, values_to=&quot;mpg&quot;) y_rep %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_line(aes(group=grp), alpha=0.2) + geom_point(data = mtcars) 3.6 Semi-parametric Model 3.6.1 Define model The linear model is a poor choice for this data, so I’ll try a model with splines next. The stan_gamm4 function from the rstanarm package fits Bayesian nonlinear (and mixed) models. mdl3 &lt;- stan_gamm4(mpg ~ s(disp, bs=&quot;cr&quot;, k=7), data = mtcars, adapt_delta = 0.99) 3.6.2 Prior Predictive Distribution Unlike the linear model, it’s not as straightforward to manually construct the prior predictive distribution. Fortunately, rstanarm can automatically generate the prior predictive distribution too–we just refit the model without conditioning on the data by setting prior_PD = TRUE. mdl3_prior_pred &lt;- stan_gamm4(mpg ~ s(disp, bs=&quot;cr&quot;, k=7), data = mtcars, prior_PD = TRUE, adapt_delta = 0.99) N &lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) prior_pred &lt;- data.frame(t(posterior_epred(mdl3_prior_pred, newdata=data.frame(disp=D), draws=N))) tmp &lt;- prior_pred %&gt;% mutate(disp = D)%&gt;% pivot_longer(cols=-&quot;disp&quot;, names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) tmp %&gt;% ggplot() + geom_line(mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg), color=&quot;blue&quot;) This prior predictive distribution gives us some crazy possibilities. However we saw earlier that there is enough data that the model isn’t very sensitive to the choice of prior, so let’s continue and see what happens. 3.6.3 Diagnostics and Posterior mcmc_trace(mdl3, regex_pars=c(&quot;disp&quot;, &quot;sigma&quot;)) summary(mdl3) ## ## Model Info: ## function: stan_gamm4 ## family: gaussian [identity] ## formula: mpg ~ s(disp, bs = &quot;cr&quot;, k = 7) ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 32 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 20.1 0.4 19.6 20.1 20.6 ## s(disp).1 0.1 1.2 -1.2 0.1 1.5 ## s(disp).2 -0.9 1.0 -2.3 -0.7 0.3 ## s(disp).3 0.0 0.5 -0.7 0.0 0.7 ## s(disp).4 1.2 0.4 0.7 1.2 1.6 ## s(disp).5 0.4 0.1 0.2 0.4 0.6 ## s(disp).6 -3.1 0.3 -3.5 -3.1 -2.8 ## sigma 2.4 0.3 2.0 2.4 2.8 ## smooth_sd[s(disp)1] 1.2 0.7 0.5 1.0 2.0 ## smooth_sd[s(disp)2] 3.6 2.0 1.7 3.1 6.1 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 20.1 0.6 19.3 20.1 20.8 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 4153 ## s(disp).1 0.0 1.0 3329 ## s(disp).2 0.0 1.0 2097 ## s(disp).3 0.0 1.0 3947 ## s(disp).4 0.0 1.0 3090 ## s(disp).5 0.0 1.0 4789 ## s(disp).6 0.0 1.0 3662 ## sigma 0.0 1.0 2754 ## smooth_sd[s(disp)1] 0.0 1.0 1222 ## smooth_sd[s(disp)2] 0.0 1.0 2170 ## mean_PPD 0.0 1.0 4207 ## log-posterior 0.1 1.0 855 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). The chains, n_eff and \\(\\widehat{R}\\) look good. In the Estimates section above, we also see the posteriors for the model parameters; there isn’t an intuitive interpretation of the spline coefficients so I’ll skip ahead to the posterior predictive distribution. 3.6.4 Posterior Predictive Distribution The expectation over the ppd is plotted below, along with a loess curve for comparison. This model is clearly a better fit to the data than the linear model. plot_nonlinear(mdl3, prob=0.95) + geom_point(mapping=aes(x=disp, y=mpg-mean(mpg)), data=mtcars) + labs(title=&quot;GAM&quot;, x=&quot;disp&quot;, y=&quot;mpg-mean(mpg)&quot;) ggplot(mapping=aes(x=disp, y=mpg-mean(mpg)), data=mtcars) + geom_point()+ stat_smooth(method=&quot;loess&quot;, level=0.95) + labs(title=&quot;LOESS&quot;) 3.7 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## other attached packages: ## [1] bayesplot_1.7.2 rstanarm_2.21.1 Rcpp_1.0.5 kableExtra_1.3.1 ## [5] gridExtra_2.3 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [9] purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 tibble_3.0.4 ## [13] ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] minqa_1.2.4 colorspace_2.0-0 ellipsis_0.3.1 ## [4] ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [7] base64enc_0.1-3 fs_1.5.0 rstudioapi_0.13 ## [10] farver_2.0.3 rstan_2.21.2 DT_0.16 ## [13] fansi_0.4.1 lubridate_1.7.9.2 xml2_1.3.2 ## [16] splines_4.0.3 codetools_0.2-16 knitr_1.30 ## [19] shinythemes_1.1.2 jsonlite_1.7.1 nloptr_1.2.2.2 ## [22] broom_0.7.2 dbplyr_2.0.0 shiny_1.5.0 ## [25] compiler_4.0.3 httr_1.4.2 backports_1.2.0 ## [28] Matrix_1.2-18 assertthat_0.2.1 fastmap_1.0.1 ## [31] cli_2.2.0 later_1.1.0.1 htmltools_0.5.0 ## [34] prettyunits_1.1.1 tools_4.0.3 igraph_1.2.6 ## [37] gtable_0.3.0 glue_1.4.2 reshape2_1.4.4 ## [40] V8_3.4.0 cellranger_1.1.0 vctrs_0.3.5 ## [43] nlme_3.1-149 crosstalk_1.1.0.1 xfun_0.19 ## [46] ps_1.4.0 lme4_1.1-26 rvest_0.3.6 ## [49] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [52] renv_0.12.0 gtools_3.8.2 statmod_1.4.35 ## [55] MASS_7.3-53 zoo_1.8-8 scales_1.1.1 ## [58] colourpicker_1.1.0 hms_0.5.3 promises_1.1.1 ## [61] parallel_4.0.3 inline_0.3.17 shinystan_2.5.0 ## [64] yaml_2.2.1 curl_4.3 loo_2.3.1 ## [67] StanHeaders_2.21.0-6 stringi_1.5.3 highr_0.8 ## [70] dygraphs_1.1.1.6 boot_1.3-25 pkgbuild_1.1.0 ## [73] rlang_0.4.9 pkgconfig_2.0.3 matrixStats_0.57.0 ## [76] evaluate_0.14 lattice_0.20-41 labeling_0.4.2 ## [79] rstantools_2.1.1 htmlwidgets_1.5.2 tidyselect_1.1.0 ## [82] processx_3.4.5 plyr_1.8.6 magrittr_2.0.1 ## [85] bookdown_0.21 R6_2.5.0 generics_0.1.0 ## [88] DBI_1.1.0 mgcv_1.8-33 pillar_1.4.7 ## [91] haven_2.3.1 withr_2.3.0 xts_0.12.1 ## [94] survival_3.2-7 modelr_0.1.8 crayon_1.3.4 ## [97] rmarkdown_2.5 grid_4.0.3 readxl_1.3.1 ## [100] callr_3.5.1 threejs_0.3.3 reprex_0.3.0 ## [103] digest_0.6.27 webshot_0.5.2 xtable_1.8-4 ## [106] httpuv_1.5.4 RcppParallel_5.0.2 stats4_4.0.3 ## [109] munsell_0.5.0 viridisLite_0.3.0 shinyjs_2.0.0 References "],["rethinking.html", "Chapter 4 rethinking 4.1 Resources 4.2 Description 4.3 Environment Setup 4.4 Linear Model 4.5 Semi-parametric Model 4.6 Extras 4.7 Session Info", " Chapter 4 rethinking 4.1 Resources Statistical Rethinking by McElreath Statistical Rethinking Lectures on YouTube rethinking github repo 4.2 Description Statistical Rethinking was the first book I read on Bayesian methods, and I highly recommend it. All of the problems in the book are done with the rethinking package. It uses the familiar formula syntax for defining models. However, unlike rstanarm the functions are not close mirrors of familiar frequentist functions. Another difference from rstanarm is you must specify all priors–there are no defaults. I ran into some difficulty with the semi-parametric regression (4.5), but aside from that it’s also a very good option for getting started. 4.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(datasets) data(mtcars) library(rethinking) 4.4 Linear Model 4.4.1 Define Model The rethinking package does not have default priors so I need to explicitly choose them. Again I’ll use the follmodel is \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim Normal(25,10^2) \\\\ b &amp;\\sim Normal(-0.2, 0.1^2) \\\\ \\sigma &amp;\\sim Exponential(1) \\end{align*}\\] # Note the sign change for mu and b, this seems to be a quirk # of map2stan that it didn&#39;t like b ~ dnorm(-0.2, 0.1) f &lt;- alist( mpg ~ dnorm(mu, sigma), mu &lt;- a - b * disp, a ~ dnorm(25, 10), b ~ dnorm(0.2, 0.1), sigma ~ dexp(1) ) # Note the default number of chains = 1, so I&#39;m explicitly setting to 4 here mdl1 &lt;- map2stan(f,mtcars, chains=4) ## Trying to compile a simple C file ## Computing WAIC 4.4.2 Prior Predictive Distribution # Plot prior predictive distribution N &lt;- 100 prior_samples &lt;- as.data.frame(extract.prior(mdl1, n=N)) D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) res &lt;- as.data.frame(apply(prior_samples, 1, function(x) x[1] - x[2] * (D-mean(mtcars$disp)))) %&gt;% mutate(disp = D) %&gt;% pivot_longer(cols=c(-&quot;disp&quot;), names_to=&quot;iter&quot;) res %&gt;% ggplot() + geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) + labs(x=&quot;disp&quot;, y=&quot;prior predictive mpg&quot;) 4.4.3 Diagnostics The traceplot function is the rethinking equivalent to mcmc_trace. traceplot(mdl1@stanfit) The trankplot function is the rethinking equivalent to the mcmc_rank_overlay. trankplot(mdl1) The trankplot function conveniently also displays the effective sample size (n_eff). But the precis function is another way to get both n_eff and \\(\\widehat{R}\\). precis(mdl1, prob=0.95) ## mean sd 2.5% 97.5% n_eff Rhat4 ## a 29.65030 1.18866 27.32351 32.02460 1429 1.000 ## b 0.04148 0.00448 0.03252 0.05042 1472 1.000 ## sigma 3.20829 0.40702 2.51923 4.07572 1673 1.003 4.4.4 Posterior Distribution The precis function above also displays both the posterior point estimate and credible interval. 4.4.5 Posterior Predictive Distribution Finally, I’ll check the posterior predictive distribution. The rethinking package includes the postcheck function which displays a plot for posterior predictive checking. postcheck(mdl1, window=nrow(mtcars)) Alternatively, I can plot the expectation of the posterior predictive distribution (i.e., \\(\\mu\\)) like I did with rstanarm . The sim function draws samples from the posterior predictive distribution, and the link function returns the linear predictor, possibly transformed by the inverse-link function. In this case, the model is a Gaussian likelihood with an identity link function, so the sim and link functions return identical results. newdata &lt;- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp))) y_rep &lt;- as.data.frame(t(link(mdl1, data=newdata, n=50))) %&gt;% cbind(newdata) %&gt;% pivot_longer(cols=starts_with(&quot;V&quot;), names_to=&quot;grp&quot;, values_to=&quot;mpg&quot;) y_rep %&gt;% ggplot(aes(x=disp, y=mpg)) + geom_line(aes(group=grp), alpha=0.2) + geom_point(data = mtcars) 4.5 Semi-parametric Model 4.5.1 Define Model Setting up the semi-parametric model is a bit more work in the rethinking package. First, I explicitly create the splines. The component splines are plotted below. library(splines) num_knots &lt;- 4 # number of interior knots knot_list &lt;- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots)) B &lt;- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df1 &lt;- cbind(disp=mtcars$disp, B) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) # Plot at smaller intervals so curves are smooth N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) B_plot &lt;- bs(D, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df2 &lt;- cbind(disp=D, B_plot) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) ggplot(mapping=aes(x=disp, y=val, color=spline)) + geom_point(data=df1) + geom_line(data=df2, linetype=&quot;dashed&quot;) Then I define the model with the splines. I wasn’t able to get this model to work with either the map2stan or ulam functions, so I used quap instead which fits a quadratic approximation. f &lt;- alist( mpg ~ dnorm(mu, sigma), mu &lt;- a - B %*% w, a ~ dnorm(25, 10), w ~ dnorm(0,5), sigma ~ dexp(1) ) mdl2 &lt;- quap(f, data=list(mpg=mtcars$mpg, B=B), start=list(w=rep(1, ncol(B))) ) 4.5.2 Prior Predictive Distribution 4.5.3 Diagnostics Since MCMC was not used to fit the model, there are no chain diagnostics to examine. 4.5.4 Posterior Distribution I can still use the precis function to look at the postrior distribution, although there’s really no intuitive interpretation for the spline weights. precis(mdl2, depth=2) ## mean sd 5.5% 94.5% ## w[1] -12.0932 2.2958 -15.762 -8.4241 ## w[2] -4.3203 2.5531 -8.401 -0.2399 ## w[3] 0.4861 2.7405 -3.894 4.8660 ## w[4] 5.8948 2.8873 1.280 10.5093 ## w[5] 2.1765 2.8839 -2.433 6.7856 ## w[6] 9.0562 2.3915 5.234 12.8782 ## a 20.1995 2.0329 16.951 23.4484 ## sigma 1.9638 0.2397 1.581 2.3469 4.5.5 Posterior Predictive Distribution And finally, the posterior predictive distribution: mu &lt;- link(mdl2) mu_mean &lt;- as.data.frame(apply(mu, 2, mean)) %&gt;% mutate(disp=mtcars$disp) colnames(mu_mean) &lt;- c(&quot;mpg_ppd&quot;, &quot;disp&quot;) mu_PI &lt;- as.data.frame(t(apply(mu,2,PI,0.95))) %&gt;% mutate(disp=mtcars$disp) colnames(mu_PI) &lt;- c(&quot;lwr&quot;, &quot;upr&quot;, &quot;disp&quot;) ggplot() + geom_point(data=mtcars, aes(x=disp, y=mpg)) + geom_line(data=mu_mean, aes(x=disp, y=mpg_ppd), color=&quot;blue&quot;) + geom_ribbon(data=mu_PI, aes(x=disp, ymin=lwr, ymax=upr), alpha=0.2) + labs(title=&quot;GAM&quot;) ggplot(mapping=aes(x=disp, y=mpg-mean(mpg)), data=mtcars) + geom_point()+ stat_smooth(method=&quot;loess&quot;, level=0.95) + labs(title=&quot;LOESS&quot;) 4.6 Extras The rethinking package has some nice extras. One is the stancode function will return the actual stan code for the model. This is a great way to start getting familiar with stan syntax! stancode(mdl1) ## //2020-12-08 13:57:09 ## data{ ## int&lt;lower=1&gt; N; ## real mpg[N]; ## real disp[N]; ## } ## parameters{ ## real a; ## real b; ## real&lt;lower=0&gt; sigma; ## } ## model{ ## vector[N] mu; ## sigma ~ exponential( 1 ); ## b ~ normal( 0.2 , 0.1 ); ## a ~ normal( 25 , 10 ); ## for ( i in 1:N ) { ## mu[i] = a - b * disp[i]; ## } ## mpg ~ normal( mu , sigma ); ## } ## generated quantities{ ## vector[N] mu; ## for ( i in 1:N ) { ## mu[i] = a - b * disp[i]; ## } ## } Also, map2stan returns an object that contains a stanfit object which you can access with the “@stanfit” accessor. Both bayesplot and shinystan work with that stanfit object just like they do with rstanarm or rstan models. 4.7 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] splines parallel stats graphics grDevices datasets utils ## [8] methods base ## ## other attached packages: ## [1] rethinking_2.13 rstan_2.21.2 StanHeaders_2.21.0-6 ## [4] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [7] purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 ## [10] tibble_3.0.4 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 jsonlite_1.7.1 modelr_0.1.8 RcppParallel_5.0.2 ## [5] assertthat_0.2.1 stats4_4.0.3 renv_0.12.0 cellranger_1.1.0 ## [9] yaml_2.2.1 pillar_1.4.7 backports_1.2.0 lattice_0.20-41 ## [13] glue_1.4.2 digest_0.6.27 rvest_0.3.6 colorspace_2.0-0 ## [17] Matrix_1.2-18 htmltools_0.5.0 pkgconfig_2.0.3 broom_0.7.2 ## [21] haven_2.3.1 bookdown_0.21 mvtnorm_1.1-1 scales_1.1.1 ## [25] processx_3.4.5 mgcv_1.8-33 farver_2.0.3 generics_0.1.0 ## [29] ellipsis_0.3.1 withr_2.3.0 cli_2.2.0 magrittr_2.0.1 ## [33] crayon_1.3.4 readxl_1.3.1 evaluate_0.14 ps_1.4.0 ## [37] fs_1.5.0 fansi_0.4.1 nlme_3.1-149 MASS_7.3-53 ## [41] xml2_1.3.2 pkgbuild_1.1.0 tools_4.0.3 loo_2.3.1 ## [45] prettyunits_1.1.1 hms_0.5.3 lifecycle_0.2.0 matrixStats_0.57.0 ## [49] V8_3.4.0 munsell_0.5.0 reprex_0.3.0 callr_3.5.1 ## [53] compiler_4.0.3 rlang_0.4.9 grid_4.0.3 rstudioapi_0.13 ## [57] labeling_0.4.2 rmarkdown_2.5 gtable_0.3.0 codetools_0.2-16 ## [61] inline_0.3.17 DBI_1.1.0 curl_4.3 R6_2.5.0 ## [65] gridExtra_2.3 lubridate_1.7.9.2 knitr_1.30 shape_1.4.5 ## [69] stringi_1.5.3 Rcpp_1.0.5 vctrs_0.3.5 dbplyr_2.0.0 ## [73] tidyselect_1.1.0 xfun_0.19 coda_0.19-4 "],["brms.html", "Chapter 5 brms 5.1 Resources 5.2 Description 5.3 Environment Setup 5.4 Linear Model 5.5 Semi-parametric Model 5.6 Semi-parametric Model (Random Walk Prior) 5.7 Session Info", " Chapter 5 brms 5.1 Resources Solomon Kurz’s translation of Statistical Rethinking 5.2 Description 5.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(datasets) data(mtcars) library(brms) library(bayesplot) 5.4 Linear Model 5.4.1 Define Model Like the rethinking package, rstan doesn’t have default priors, so I need to explicitly choose them: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim N(25,10) \\\\ b &amp;\\sim N(-0.2, 0.1) \\\\ \\sigma &amp;\\sim Exp(1) \\end{align*}\\] 5.4.2 Prior Predictive Distribution 5.4.3 Diagnostics 5.4.4 Posterior Distribution 5.4.5 Posterior Predictive Distribution 5.5 Semi-parametric Model 5.5.1 Define Model First, I’ll define the splines just as I did with the rethinking package. library(splines) num_knots &lt;- 4 # number of interior knots knot_list &lt;- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots)) B &lt;- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df1 &lt;- cbind(disp=mtcars$disp, B) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) # Plot at smaller intervals so curves are smooth N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) B_plot &lt;- bs(D, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df2 &lt;- cbind(disp=D, B_plot) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) ggplot(mapping=aes(x=disp, y=val, color=spline)) + geom_point(data=df1) + geom_line(data=df2, linetype=&quot;dashed&quot;) Note: the dashed lines are the splines and the points are the values of the spline at the specific values of mtcars$disp; the points are inputs into the stan model. 5.5.2 Prior Predictive Distribution 5.5.3 Diagnostics 5.5.4 Posterior Distribution 5.5.5 Posterior Predictive Distribution 5.6 Semi-parametric Model (Random Walk Prior) 5.6.1 Define Model 5.6.2 Prior Predictive Distribution 5.6.3 Diagnostics 5.6.4 Posterior Distribution 5.6.5 Posterior Predictive Distribution 5.7 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] splines stats graphics grDevices datasets utils methods ## [8] base ## ## other attached packages: ## [1] bayesplot_1.7.2 brms_2.14.4 Rcpp_1.0.5 forcats_0.5.0 ## [5] stringr_1.4.0 dplyr_1.0.2 purrr_0.3.4 readr_1.4.0 ## [9] tidyr_1.1.2 tibble_3.0.4 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] minqa_1.2.4 colorspace_2.0-0 ellipsis_0.3.1 ## [4] ggridges_0.5.2 rsconnect_0.8.16 markdown_1.1 ## [7] base64enc_0.1-3 fs_1.5.0 rstudioapi_0.13 ## [10] farver_2.0.3 rstan_2.21.2 DT_0.16 ## [13] fansi_0.4.1 mvtnorm_1.1-1 lubridate_1.7.9.2 ## [16] xml2_1.3.2 codetools_0.2-16 bridgesampling_1.0-0 ## [19] knitr_1.30 shinythemes_1.1.2 projpred_2.0.2 ## [22] jsonlite_1.7.1 nloptr_1.2.2.2 broom_0.7.2 ## [25] dbplyr_2.0.0 shiny_1.5.0 compiler_4.0.3 ## [28] httr_1.4.2 backports_1.2.0 assertthat_0.2.1 ## [31] Matrix_1.2-18 fastmap_1.0.1 cli_2.2.0 ## [34] later_1.1.0.1 prettyunits_1.1.1 htmltools_0.5.0 ## [37] tools_4.0.3 igraph_1.2.6 coda_0.19-4 ## [40] gtable_0.3.0 glue_1.4.2 reshape2_1.4.4 ## [43] V8_3.4.0 cellranger_1.1.0 vctrs_0.3.5 ## [46] nlme_3.1-149 crosstalk_1.1.0.1 xfun_0.19 ## [49] ps_1.4.0 lme4_1.1-26 rvest_0.3.6 ## [52] mime_0.9 miniUI_0.1.1.1 lifecycle_0.2.0 ## [55] renv_0.12.0 gtools_3.8.2 statmod_1.4.35 ## [58] MASS_7.3-53 zoo_1.8-8 scales_1.1.1 ## [61] colourpicker_1.1.0 hms_0.5.3 promises_1.1.1 ## [64] Brobdingnag_1.2-6 parallel_4.0.3 inline_0.3.17 ## [67] shinystan_2.5.0 curl_4.3 gamm4_0.2-6 ## [70] yaml_2.2.1 gridExtra_2.3 StanHeaders_2.21.0-6 ## [73] loo_2.3.1 stringi_1.5.3 dygraphs_1.1.1.6 ## [76] pkgbuild_1.1.0 boot_1.3-25 rlang_0.4.9 ## [79] pkgconfig_2.0.3 matrixStats_0.57.0 evaluate_0.14 ## [82] lattice_0.20-41 labeling_0.4.2 rstantools_2.1.1 ## [85] htmlwidgets_1.5.2 processx_3.4.5 tidyselect_1.1.0 ## [88] plyr_1.8.6 magrittr_2.0.1 bookdown_0.21 ## [91] R6_2.5.0 generics_0.1.0 DBI_1.1.0 ## [94] pillar_1.4.7 haven_2.3.1 withr_2.3.0 ## [97] mgcv_1.8-33 xts_0.12.1 abind_1.4-5 ## [100] modelr_0.1.8 crayon_1.3.4 rmarkdown_2.5 ## [103] grid_4.0.3 readxl_1.3.1 callr_3.5.1 ## [106] threejs_0.3.3 reprex_0.3.0 digest_0.6.27 ## [109] xtable_1.8-4 httpuv_1.5.4 RcppParallel_5.0.2 ## [112] stats4_4.0.3 munsell_0.5.0 shinyjs_2.0.0 "],["rstan.html", "Chapter 6 rstan 6.1 Resources 6.2 Description 6.3 Environment Setup 6.4 Linear Model 6.5 Semi-parametric Model 6.6 Semi-parametric Model (Random Walk Prior) 6.7 Session Info", " Chapter 6 rstan 6.1 Resources mc-stan online documentation 6.2 Description All of the packages in the previous chapters are running stan under the hood. One of the biggest advantages to using rstan (or cmdstanr) is you get the full power and flexibility of stan so you can build models that aren’t supported by the other packages. The tradeoff is syntax is quite different which can be a challenge for those who are only familiar with R. 6.3 Environment Setup rm(list=ls()) set.seed(123) options(&quot;scipen&quot; = 1, &quot;digits&quot; = 4) library(tidyverse) library(datasets) data(mtcars) library(rstan) library(bayesplot) # Saves compiled version of model so it only has to be recompiled if the model is changed rstan_options(auto_write = TRUE) # Set number of cores options(mc.cores = parallel::detectCores()-1) 6.4 Linear Model 6.4.1 Define Model Like the rethinking package, rstan doesn’t have default priors, so I need to explicitly choose them: \\[\\begin{align*} mpg &amp;\\sim N(\\mu, \\sigma^2) \\\\ \\mu &amp;= a + b*disp \\\\ a &amp;\\sim N(25,10) \\\\ b &amp;\\sim N(-0.2, 0.1) \\\\ \\sigma &amp;\\sim Exp(1) \\end{align*}\\] For a simple linear model there are three sections to the model definition: data - This is where the data structures for the known/observed portions of the model (e.g., the number of observations, the number and type of predictors) are defined. parameters - This is where the data structures for the parameters to be estimated are defined. For example, the coefficients of the simple linear model belong in this section. model - This is where the model (including priors) is defined using the data structures from the previous sections. # Define model mdl_code &lt;- &#39; data{ int&lt;lower=1&gt; N; vector[N] mpg; vector[N] disp; } parameters{ real a; real b; real&lt;lower=0.0&gt; sigma; } model{ // Likelihood mpg ~ normal(a + b * disp, sigma); // Priors a ~ normal(25, 10); b ~ normal(-0.2, 0.1); sigma ~ exponential(1); } &#39; A couple of comments about the model definition. For those only familiar with R, it may seem like a lot of extra “stuff” is going on in the data and parameters sections. This is because stan is written in C++ which is statically typed, unlike R and Python which are dynamically typed. Essentially what that means is you must define the type of any variable before you use it. The lower= (and upper= not shown in this example) options define bounds for a variable. The data is checked against the bounds which can detect errors pre-compilation. Generally, bounds are a good idea but aren’t required. Next, populate the data structures from the data section and save in a list. mdl_data &lt;- list(N = nrow(mtcars), mpg = mtcars$mpg, disp = mtcars$disp) And this is the call to fit the model. # Fit model mdl1 &lt;- stan(model_code=mdl_code, data=mdl_data, model_name=&quot;mdl1&quot;) ## Trying to compile a simple C file 6.4.2 Prior Predictive Distribution I could manually construct the prior predictive distribution like I did in 4.4.2. Instead I’ll have stan generate the prior predictive distribution which will be useful for more complex models. First, create another model with just the data and generated quantities section. The generated quantities section mirrors the model section except it is now drawing samples from the priors without conditioning on the observed data. Also, in the stan call set the sampling algorithm for fixed parameters. # Plot prior predictive distribution mdl_prior &lt;- &#39; data{ int&lt;lower=1&gt; N; vector[N] disp; } generated quantities{ real a_sim = normal_rng(25, 10); real b_sim = uniform_rng(-0.1, 0.0); real sigma_sim = exponential_rng(0.2); real mpg_sim[N] = normal_rng(a_sim + b_sim * disp, sigma_sim); } &#39; N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) mdl_data_prior &lt;- list(N = N, disp=D) mdl_prior &lt;- stan(model_code=mdl_prior, data=mdl_data_prior, model_name=&quot;mdl_prior&quot;, chains=1, algorithm=&quot;Fixed_param&quot;) ## Trying to compile a simple C file draws &lt;- as.data.frame(mdl_prior) %&gt;% head(100) # Expected value prior predictive distribution exp_mpg_sim &lt;- apply(draws, 1, function(x) x[&quot;a_sim&quot;] + x[&quot;b_sim&quot;]*D) %&gt;% as.data.frame() %&gt;% mutate(disp = D) %&gt;% pivot_longer(-c(&quot;disp&quot;), names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) ggplot() + geom_line(data=exp_mpg_sim, mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) 6.4.3 Diagnostics mcmc_rank_overlay(mdl1, pars=c(&quot;a&quot;, &quot;b&quot;, &quot;sigma&quot;)) print(mdl1) ## Inference for Stan model: mdl1. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a 29.58 0.03 1.21 27.17 28.80 29.56 30.39 31.93 1394 1 ## b -0.04 0.00 0.00 -0.05 -0.04 -0.04 -0.04 -0.03 1444 1 ## sigma 3.19 0.01 0.40 2.53 2.92 3.15 3.41 4.07 1553 1 ## lp__ -57.58 0.04 1.28 -60.90 -58.16 -57.23 -56.64 -56.15 1298 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Dec 8 13:58:47 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 6.4.4 Posterior Distribution The print function above displays information about the posterior distributions in addition to n_eff. Alternatively, the plot function provides a graphical display of the posterior distributions. plot(mdl1, ci_level=0.89) ## ci_level: 0.89 (89% intervals) ## outer_level: 0.95 (95% intervals) 6.4.5 Posterior Predictive Distribution Using the posterior samples, I can plot the expected value of the posterior predictive distribution. N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) draws &lt;- as.data.frame(mdl1) %&gt;% head(100) # Expected value posterior predictive distribution post_pred &lt;- apply(draws, 1, function(x) x[&quot;a&quot;] + x[&quot;b&quot;]*D) %&gt;% as.data.frame() %&gt;% mutate(disp = D) %&gt;% pivot_longer(-c(&quot;disp&quot;), names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) ggplot() + geom_line(data=post_pred, mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg, color=factor(cyl))) Note that the expected value of the ppd doesn’t include \\(\\sigma\\). An alternative is to have stan automatically generate samples from the posterior predictive distribution by adding a generated quantities section to the model (similar to what I did for the prior predictive distribution). # Define model mdl_code_ppd &lt;- &#39; data{ int&lt;lower=1&gt; N; vector[N] mpg; vector[N] disp; } parameters{ real a; real&lt;lower=-0.1, upper=0.0&gt; b; real&lt;lower=0.0&gt; sigma; } transformed parameters{ vector[N] Y_hat = a + b * disp; } model{ // Likelihood mpg ~ normal(Y_hat, sigma); // Priors a ~ normal(25, 10); b ~ uniform(-0.1, 0.0); sigma ~ exponential(0.2); } generated quantities{ // Posterior Predictive real mpg_ppd[N] = normal_rng(Y_hat, sigma); } &#39; # Fit model mdl1_ppd &lt;- stan(model_code=mdl_code_ppd, data=mdl_data) ## Trying to compile a simple C file draws &lt;- as.data.frame(mdl1_ppd) # 95% credible interval for expected value of ppd Eppd &lt;- draws %&gt;% select(starts_with(&quot;Y_hat&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) # 95 credible interval for ppd ppd &lt;- draws %&gt;% select(starts_with(&quot;mpg_ppd&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp=mdl_data$disp) ggplot() + geom_line(data=Eppd, mapping=aes(x=disp, y=`50%`)) + geom_ribbon(data=ppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;lightblue&quot;) + geom_ribbon(data=Eppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;dodgerblue&quot;) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg)) + labs(x=&quot;disp&quot;, y=&quot;mpg&quot;) The darker blue area is a 95% credible interval for the expected value of the posterior predictive distribution and the lighter blue area is the 95% credible interval for the posterior predictive distribution. 6.5 Semi-parametric Model 6.5.1 Define Model First, I’ll define the splines just as I did with the rethinking package. library(splines) num_knots &lt;- 4 # number of interior knots knot_list &lt;- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots)) B &lt;- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df1 &lt;- cbind(disp=mtcars$disp, B) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) # Plot at smaller intervals so curves are smooth N&lt;- 50 D &lt;- seq(min(mtcars$disp), max(mtcars$disp), length.out = N) B_plot &lt;- bs(D, knots=knot_list[-c(1,num_knots)], intercept=TRUE) df2 &lt;- cbind(disp=D, B_plot) %&gt;% as.data.frame() %&gt;% pivot_longer(-disp, names_to=&quot;spline&quot;, values_to=&quot;val&quot;) ggplot(mapping=aes(x=disp, y=val, color=spline)) + geom_point(data=df1) + geom_line(data=df2, linetype=&quot;dashed&quot;) Note: the dashed lines are the splines and the points are the values of the spline at the specific values of mtcars$disp; the points are inputs into the stan model. # Define model mdl_code &lt;- &#39; data{ int&lt;lower=1&gt; N; int&lt;lower=1&gt; num_basis; vector[N] mpg; vector[N] disp; matrix[N, num_basis] B; } parameters{ real a; real&lt;lower=0.0&gt; sigma; vector[num_basis] w; } transformed parameters{ vector[N] Y_hat = a + B*w; } model{ // Likelihood mpg ~ normal(Y_hat, sigma); // Priors a ~ normal(25, 10); sigma ~ exponential(0.2); w ~ normal(0, 5); } generated quantities{ // Posterior Predictive real mpg_ppd[N] = normal_rng(Y_hat, sigma); } &#39; mdl_data &lt;- list(N=nrow(mtcars), num_basis=ncol(B), B=B, mpg = mtcars$mpg, disp = mtcars$disp) # Fit model mdl1_gam &lt;- stan(model_code=mdl_code, data=mdl_data) ## Trying to compile a simple C file 6.5.2 Prior Predictive Distribution # Define model mdl2 &lt;- &#39; data{ int&lt;lower=1&gt; N; int&lt;lower=1&gt; num_basis; //vector[N] mpg; //vector[N] disp; matrix[N, num_basis] B; } generated quantities{ real a_sim = normal_rng(25, 10); real sigma_sim = exponential_rng(0.2); real mpg_sim[N]; vector[N] Y_hat; vector[num_basis] w_sim; for (i in 1:num_basis) w_sim[i] = normal_rng(0,5); Y_hat = a_sim + B * w_sim; mpg_sim = normal_rng(Y_hat, sigma_sim); } &#39; mdl_gam_prior &lt;- stan(model_code=mdl2, data=list(N=N, num_basis=ncol(B_plot), B=B_plot), chains=1, algorithm=&quot;Fixed_param&quot;) ## Trying to compile a simple C file draws &lt;- as.data.frame(mdl_gam_prior) %&gt;% head(50) # Expected value prior predictive distribution exp_mpg_sim &lt;- apply(draws, 1, function(x) { x[&quot;a_sim&quot;] + B_plot %*% x[grepl(&quot;w&quot;, names(x))] }) %&gt;% as.data.frame() %&gt;% mutate(disp = D) %&gt;% pivot_longer(-c(&quot;disp&quot;), names_to=&quot;iter&quot;, values_to=&quot;mpg&quot;) # 95% interval prior predictive distribution mpg_sim &lt;- as.data.frame(mdl_gam_prior) %&gt;% select(starts_with(&quot;mpg&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = D) ggplot() + geom_line(data=exp_mpg_sim, mapping=aes(x=disp, y=mpg, group=iter), alpha=0.2) + geom_ribbon(data=mpg_sim, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;lightblue&quot;) 6.5.3 Diagnostics # Note that bayesplot methods support tidy selection of parameters mcmc_rank_overlay(mdl1_gam, pars=vars(a, sigma, starts_with(&quot;w&quot;))) # This is the print.stanfit method and pars must be a character vector print(mdl1_gam, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) ## Inference for Stan model: 7ae6bef6fd6e1ad16f99577e64710ecd. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## a 20.23 0.07 2.05 16.31 18.84 20.22 21.61 24.31 954 1 ## sigma 2.32 0.01 0.34 1.77 2.07 2.27 2.52 3.12 2290 1 ## w[1] 11.82 0.07 2.39 7.06 10.20 11.80 13.42 16.49 1215 1 ## w[2] 4.31 0.08 2.76 -1.22 2.45 4.34 6.19 9.73 1263 1 ## w[3] -0.61 0.08 2.88 -6.09 -2.58 -0.63 1.32 5.05 1451 1 ## w[4] -5.67 0.08 3.10 -11.84 -7.78 -5.62 -3.58 0.46 1359 1 ## w[5] -2.39 0.08 2.99 -8.41 -4.41 -2.33 -0.38 3.46 1567 1 ## w[6] -8.79 0.07 2.54 -13.63 -10.55 -8.80 -7.08 -3.70 1339 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Dec 8 14:00:36 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 6.5.4 Posterior Distribution plot(mdl1_gam, pars=c(&quot;a&quot;, &quot;sigma&quot;, &quot;w&quot;)) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) 6.5.5 Posterior Predictive Distribution # 95% credible interval expected value of posterior predictive Eppd &lt;- as.data.frame(mdl1_gam) %&gt;% select(starts_with(&quot;Y_hat&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) # 95% credible interval posterior predictive ppd &lt;- as.data.frame(mdl1_gam) %&gt;% select(starts_with(&quot;mpg&quot;)) %&gt;% apply(2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))) %&gt;% t() %&gt;% as.data.frame() %&gt;% mutate(disp = mtcars$disp) ggplot() + geom_line(data=Eppd, mapping=aes(x=disp, y=`50%`)) + geom_ribbon(data=ppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;lightblue&quot;) + geom_ribbon(data=Eppd, mapping=aes(x=disp, ymin=`2.5%`, ymax=`97.5%`), alpha=0.5, fill=&quot;dodgerblue&quot;) + geom_point(data=mtcars, mapping=aes(x=disp, y=mpg)) + labs(x=&quot;disp&quot;, y=&quot;mpg&quot;) 6.6 Semi-parametric Model (Random Walk Prior) 6.6.1 Define Model 6.6.2 Prior Predictive Distribution 6.6.3 Diagnostics 6.6.4 Posterior Distribution 6.6.5 Posterior Predictive Distribution 6.7 Session Info sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] splines stats graphics grDevices datasets utils methods ## [8] base ## ## other attached packages: ## [1] bayesplot_1.7.2 rstan_2.21.2 StanHeaders_2.21.0-6 ## [4] forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [7] purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 ## [10] tibble_3.0.4 ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.2 jsonlite_1.7.1 modelr_0.1.8 RcppParallel_5.0.2 ## [5] assertthat_0.2.1 stats4_4.0.3 renv_0.12.0 cellranger_1.1.0 ## [9] yaml_2.2.1 pillar_1.4.7 backports_1.2.0 glue_1.4.2 ## [13] digest_0.6.27 rvest_0.3.6 colorspace_2.0-0 htmltools_0.5.0 ## [17] plyr_1.8.6 pkgconfig_2.0.3 broom_0.7.2 haven_2.3.1 ## [21] bookdown_0.21 scales_1.1.1 processx_3.4.5 farver_2.0.3 ## [25] generics_0.1.0 ellipsis_0.3.1 withr_2.3.0 cli_2.2.0 ## [29] magrittr_2.0.1 crayon_1.3.4 readxl_1.3.1 evaluate_0.14 ## [33] ps_1.4.0 fs_1.5.0 fansi_0.4.1 xml2_1.3.2 ## [37] pkgbuild_1.1.0 tools_4.0.3 loo_2.3.1 prettyunits_1.1.1 ## [41] hms_0.5.3 lifecycle_0.2.0 matrixStats_0.57.0 V8_3.4.0 ## [45] munsell_0.5.0 reprex_0.3.0 callr_3.5.1 compiler_4.0.3 ## [49] rlang_0.4.9 grid_4.0.3 ggridges_0.5.2 rstudioapi_0.13 ## [53] labeling_0.4.2 rmarkdown_2.5 gtable_0.3.0 codetools_0.2-16 ## [57] inline_0.3.17 DBI_1.1.0 curl_4.3 reshape2_1.4.4 ## [61] R6_2.5.0 gridExtra_2.3 lubridate_1.7.9.2 knitr_1.30 ## [65] stringi_1.5.3 parallel_4.0.3 Rcpp_1.0.5 vctrs_0.3.5 ## [69] dbplyr_2.0.0 tidyselect_1.1.0 xfun_0.19 "],["references.html", "References", " References "]]
